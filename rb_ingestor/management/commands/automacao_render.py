# rb_ingestor/management/commands/automacao_render_fixed.py
"""
Comando de automa√ß√£o CORRIGIDO para Render
Vers√£o que busca not√≠cias espec√≠ficas e gera conte√∫do baseado nelas
"""
import os
import sys
import django
from django.core.management.base import BaseCommand
from django.apps import apps
from django.utils import timezone
from django.utils.html import strip_tags
from slugify import slugify
from datetime import datetime, timedelta
import random
import logging

# Configurar logging
logger = logging.getLogger(__name__)

class Command(BaseCommand):
    help = "Automa√ß√£o CORRIGIDA que busca not√≠cias espec√≠ficas e gera conte√∫do baseado nelas"

    def add_arguments(self, parser):
        parser.add_argument("--limit", type=int, default=3, help="N√∫mero de artigos a criar")
        parser.add_argument("--force", action="store_true", help="For√ßa execu√ß√£o")
        parser.add_argument("--debug", action="store_true", help="Modo debug")

    def handle(self, *args, **options):
        Noticia = apps.get_model("rb_noticias", "Noticia")
        Categoria = apps.get_model("rb_noticias", "Categoria")
        
        self.stdout.write("=== AUTOMACAO RENDER RADARBR CORRIGIDA ===")
        self.stdout.write(f"Executado em: {timezone.now()}")
        
        # Verificar se deve executar
        if not options["force"] and not self._should_execute():
            self.stdout.write("PULANDO - timing n√£o otimizado")
            return
        
        # Obter not√≠cias espec√≠ficas (n√£o apenas t√≥picos)
        news_articles = self._get_specific_news()
        if not news_articles:
            self.stdout.write("ERRO: Nenhuma not√≠cia espec√≠fica encontrada")
            return
        
        # Executar automa√ß√£o baseada em not√≠cias reais
        created_count = self._execute_automation_from_news(news_articles, Noticia, Categoria, options["limit"])
        
        # Resultado
        self.stdout.write(self.style.SUCCESS(f"OK: {created_count} not√≠cias criadas"))
        
        # Ping sitemap se criou algo
        if created_count > 0:
            self._ping_sitemap()

    def _should_execute(self):
        """Verifica se deve executar baseado em timing"""
        Noticia = apps.get_model("rb_noticias", "Noticia")
        
        # Verificar not√≠cias recentes (√∫ltimas 3 horas)
        recent_count = Noticia.objects.filter(
            criado_em__gte=timezone.now() - timedelta(hours=3)
        ).count()
        
        # Executar se menos de 2 not√≠cias recentes
        return recent_count < 2

    def _get_specific_news(self):
        """Busca not√≠cias via RSS feeds do Google News"""
        try:
            from rb_ingestor.news_content_extractor import NewsContentExtractor
            
            extractor = NewsContentExtractor()
            topics = [
                'economia', 'pol√≠tica', 'tecnologia', 'brasil', 'mundo', 'esportes', 'sa√∫de', 'ci√™ncia'
            ]
            
            processed_news = []
            for topic in topics:
                self.stdout.write(f"üîç Buscando RSS para: {topic}")
                items = extractor.get_rss_items_for_topic(topic, max_items=2)
                
                for item in items:
                    title = item.get('title') or ''
                    desc = item.get('description') or ''
                    url = item.get('link') or ''
                    
                    if title and url and self._is_valid_news_article({'title': title, 'url': url}):
                        processed_news.append({
                            'title': title,
                            'description': desc,
                            'url': url,
                            'published_date': '',
                            'source': '',
                            'topic': self._extract_main_topic(title)
                        })
                        
                        if len(processed_news) >= 8:  # Limite de artigos
                            break
                
                if len(processed_news) >= 8:
                    break
            
            self.stdout.write(f"‚úÖ RSS retornou {len(processed_news)} not√≠cias")
            return processed_news
            
        except Exception as e:
            self.stdout.write(f"‚ö† Erro RSS: {e}")
            return []

    def _get_specific_news_fallback_rss(self):
        """Fallback: usa RSS do Google News via NewsContentExtractor para montar artigos espec√≠ficos."""
        try:
            from rb_ingestor.news_content_extractor import NewsContentExtractor
            extractor = NewsContentExtractor()
            topics = [
                'economia', 'pol√≠tica', 'tecnologia', 'brasil', 'mundo', 'esportes', 'sa√∫de', 'ci√™ncia'
            ]
            processed = []
            for topic in topics:
                items = extractor.get_rss_items_for_topic(topic, max_items=3) or []
                for item in items:
                    title = item.get('title') or ''
                    desc = item.get('description') or ''
                    url = item.get('link') or ''
                    if not title or not url:
                        continue
                    processed.append({
                        'title': title,
                        'description': desc,
                        'url': url,
                        'published_date': '',
                        'source': '',
                        'topic': self._extract_main_topic(title)
                    })
                    if len(processed) >= 10:
                        break
                if len(processed) >= 10:
                    break
            return processed
        except Exception as e:
            self.stdout.write(f"‚ö† Erro no fallback RSS: {e}")
            return []

    def _is_valid_news_article(self, article):
        """Verifica se √© uma not√≠cia v√°lida"""
        title = article.get('title', '')
        description = article.get('description', '')
        
        # Verificar se tem t√≠tulo e descri√ß√£o
        if not title or not description:
            return False
        
        # Verificar tamanho m√≠nimo
        if len(title) < 20 or len(description) < 50:
            return False
        
        # Verificar se n√£o √© muito gen√©rico
        generic_words = ['not√≠cias', '√∫ltimas', 'hoje', 'agora', 'atualiza√ß√µes']
        if any(word in title.lower() for word in generic_words):
            return False
        
        return True

    def _extract_main_topic(self, title):
        """Extrai o t√≥pico principal do t√≠tulo"""
        # Palavras comuns para remover
        common_words = [
            'no', 'do', 'da', 'em', 'para', 'com', 'por', 'que', '√©', 'foi', 
            'ser', 'ter', 'h√°', 'mais', 'menos', 'sobre', 'ap√≥s', 'durante', 
            'entre', 'at√©', 'desde', 'a', 'o', 'as', 'os', 'um', 'uma', 'uns', 
            'umas', 'de', 'e', 'ou', 'mas', 'se', 'n√£o', 'j√°', 'ainda', 'tamb√©m', 
            's√≥', 'muito', 'pouco', 'todo', 'toda', 'todos', 'todas', 'cada', 
            'qual', 'quando', 'onde', 'como', 'porque', 'porqu√™', 'por que', 'por qu√™'
        ]
        
        # Limpar t√≠tulo
        title_clean = title.lower()
        words = title_clean.split()
        
        # Remover palavras comuns
        relevant_words = [word for word in words if word not in common_words and len(word) > 3]
        
        if relevant_words:
            # Pegar as 2-3 palavras mais relevantes
            topic = ' '.join(relevant_words[:3])
            return topic
        
        return title[:50]  # Fallback para primeiras 50 caracteres

    def _execute_automation_from_news(self, news_articles, Noticia, Categoria, limit):
        """Executa a automa√ß√£o baseada em not√≠cias espec√≠ficas"""
        created_count = 0
        
        for i, article in enumerate(news_articles[:limit]):
            try:
                # Resolver URL original do Google News (se for link do GN)
                original_url = self._resolve_original_url(article.get('url', ''))
                if original_url:
                    article['url'] = original_url
                    article['original_url'] = original_url
                # Gerar conte√∫do baseado na not√≠cia espec√≠fica
                title, content = self._generate_content_from_news(article)
                
                # Categorizar baseado no conte√∫do da not√≠cia
                categoria = self._get_category_from_news(article, Categoria)
                
                # Verificar duplicatas
                if self._check_duplicate(title, Noticia):
                    self.stdout.write(f"‚ö† Pulando duplicata: {title}")
                    continue
                
                # Criar not√≠cia
                # Garantir unicidade de slug e fonte_url
                ts = timezone.now().strftime('%Y%m%d%H%M%S')
                fonte_url_value = article.get('url') or f"render-automation-{ts}-{i}"
                # Se j√° existir, anexar sufixo √∫nico
                if Noticia.objects.filter(fonte_url=fonte_url_value).exists():
                    fonte_url_value = f"{fonte_url_value}?t={ts}{i}"

                noticia = Noticia.objects.create(
                    titulo=title,
                    slug=f"{slugify(title)[:120]}-{ts}",
                    conteudo=content,
                    publicado_em=timezone.now(),
                    categoria=categoria,
                    fonte_url=fonte_url_value,
                    fonte_nome=article.get('source', 'RadarBR Automation'),
                    status=1  # PUBLICADO
                )
                
                # Adicionar imagem espec√≠fica
                self._add_specific_image(noticia, article)
                
                created_count += 1
                self.stdout.write(f"‚úì Criado: {title}")
                
            except Exception as e:
                self.stdout.write(f"‚ùå Erro ao criar not√≠cia: {e}")
                continue
        
        return created_count

    def _resolve_original_url(self, url: str) -> str:
        """Resolve URL do Google News para o link do ve√≠culo original."""
        try:
            if not url:
                return ""
            if 'news.google.' not in url:
                return url
            import requests
            from urllib.parse import urlparse, parse_qs
            # 1) Query param "url"
            try:
                parsed = urlparse(url)
                qs = parse_qs(parsed.query)
                if 'url' in qs and qs['url']:
                    candidate = qs['url'][0]
                    if candidate and 'news.google.' not in candidate:
                        return candidate
            except Exception:
                pass
            # 2) Follow redirects
            session = requests.Session()
            session.headers.update({
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            })
            resp = session.get(url, timeout=20, allow_redirects=True)
            resp.raise_for_status()
            final_url = resp.url
            if final_url and 'news.google.' not in final_url:
                return final_url
        except Exception:
            return ""
        return ""

    def _generate_content_from_news(self, article):
        """Gera conte√∫do usando RSS + navegador headless para extrair conte√∫do real"""
        try:
            # 1) Extrair conte√∫do real usando navegador headless
            base_words = None
            enhanced_article = article.copy()
            
            try:
                from rb_ingestor.news_content_extractor import NewsContentExtractor
                extractor = NewsContentExtractor()
                
                # Tentar extrair do URL do RSS
                rss_url = article.get('url', '')
                if rss_url:
                    self.stdout.write(f"üîç Extraindo conte√∫do real de: {rss_url}")
                    data = extractor.extract_content_from_url(rss_url)
                    
                    if data and data.get('content'):
                        base_words = len((data['content'] or '').split())
                        self.stdout.write(f"‚úÖ Conte√∫do completo extra√≠do: {base_words} palavras")
                        
                        # Atualizar artigo com dados reais
                        enhanced_article.update({
                            'title': data.get('title', article.get('title', '')),
                            'description': data.get('description', article.get('description', '')),
                            'content': data.get('content', ''),
                            'author': data.get('author', ''),
                            'date': data.get('date', ''),
                            'real_content': True,
                            'source_domain': data.get('source_domain', ''),
                            'original_url': rss_url
                        })
                    else:
                        self.stdout.write("‚ö† Falha ao extrair conte√∫do real, usando dados RSS")
                else:
                    self.stdout.write("‚ö† Sem URL RSS para extrair")
                    
            except Exception as e:
                self.stdout.write(f"‚ö† Erro na extra√ß√£o com navegador: {e}")
            
            # 2) Definir palavras m√≠nimas baseado no conte√∫do extra√≠do
            if base_words and base_words >= 100:
                min_words = max(600, int(base_words * 0.7))  # 70% do conte√∫do original
                self.stdout.write(f"üìä Base real: {base_words} palavras ‚Üí Alvo: {min_words} palavras")
            else:
                min_words = 700  # Padr√£o quando n√£o h√° conte√∫do real
                self.stdout.write(f"üìä Sem conte√∫do real ‚Üí Alvo padr√£o: {min_words} palavras")
            
            # 3) Gerar conte√∫do com IA usando dados reais
            from rb_ingestor.ai_enhanced import generate_enhanced_article
            
            ai_content = generate_enhanced_article(
                article.get('topic', ''), 
                enhanced_article, 
                min_words
            )
            
            if not ai_content:
                self.stdout.write("‚ö† IA n√£o retornou conte√∫do")
                raise RuntimeError("IA n√£o retornou conte√∫do")
            
            # 4) Processar resultado
            title = strip_tags(ai_content.get("title", enhanced_article.get('title', '')))[:200]
            html = ai_content.get("html", "<p></p>")
            content = f'<p class="dek">{strip_tags(ai_content.get("dek", enhanced_article.get('description', '')))[:220]}</p>\n{html}'
            
            # 5) Contar palavras e validar
            from django.utils.html import strip_tags as dj_strip
            word_count = len(dj_strip(content).split())
            quality_score = ai_content.get('quality_score', 0)
            
            self.stdout.write(f"‚úÖ IA gerou {word_count} palavras (qualidade: {quality_score}%)")
            
            # 6) Verificar margem
            if min_words * 0.85 <= word_count <= min_words * 1.15:
                self.stdout.write(f"‚úÖ Conte√∫do dentro da margem ideal: {word_count} palavras")
            else:
                self.stdout.write(f"‚ö† Conte√∫do fora da margem ideal: {word_count} palavras")
            
            return title, content
            
        except Exception as e:
            self.stdout.write(f"‚ùå Publica√ß√£o cancelada: {e}")
            # Propagar exce√ß√£o para que o caller pule este artigo
            raise

    def _normalize_two_h2_sections(self, html: str) -> str:
        """Garante exatamente 2 H2: mant√©m os dois primeiros; H2 adicionais viram H3.
        Se n√£o houver 2 H2, insere subt√≠tulos padr√£o ap√≥s o primeiro par√°grafo.
        """
        try:
            if not html:
                return html
            import re
            # Demover H2 extras para H3, preservando os 2 primeiros
            positions = [m.start() for m in re.finditer(r"<h2\b", html, flags=re.IGNORECASE)]
            if len(positions) >= 2:
                # Substituir a partir do 3¬∫ <h2 ...> por <h3 ...>
                def replacer(match, _state={"count": 0}):
                    _state["count"] += 1
                    if _state["count"] <= 2:
                        return match.group(0)
                    return match.group(0).replace("<h2", "<h3").replace("</h2>", "</h3>")
                # Primeiro, marcar blocos de fechamento tamb√©m
                html = re.sub(r"</h2>", "</h2>", html, flags=re.IGNORECASE)
                html = re.sub(r"<h2(?=[^>]*>)", replacer, html, flags=re.IGNORECASE)
                # Corrigir fechamentos residuais de H2 extras
                html = re.sub(r"</h2>(?=(?:[^<]|<(?!/?h2))*$)", "</h3>", html, flags=re.IGNORECASE)
                return html

            # Se houver 0 ou 1 H2, inserir dois subt√≠tulos padr√£o ap√≥s o primeiro </p>
            insert_at = html.lower().find("</p>")
            insert_at = (insert_at + 4) if insert_at != -1 else 0
            h2_block = (
                "\n<h2>Contexto e Principais Pontos</h2>\n"
                "<p>Entenda os elementos centrais do caso com fatos e informa√ß√µes claras ao leitor.</p>\n"
                "<h2>Desdobramentos e Impactos</h2>\n"
                "<p>Resumo dos efeitos pr√°ticos, rea√ß√µes e pr√≥ximos passos relacionados ao tema.</p>\n"
            )
            return html[:insert_at] + h2_block + html[insert_at:]
        except Exception:
            return html

    def _generate_title_from_news(self, article):
        """T√≠tulo PR√ìPRIO otimizado para SEO, inspirado no original sem copiar."""
        import re
        topic = article.get('topic', '') or ''
        original = (article.get('title') or '').strip()
        description = (article.get('description') or '').strip()

        # Limpeza de marcas/portais
        portals = ['G1','Globo','Folha','Estad√£o','UOL','Terra','R7','IG','Exame','Metr√≥poles','O Globo','CNN','BBC','Reuters']
        clean = original
        for p in portals:
            clean = clean.replace(f' - {p}', '').replace(f' | {p}', '').replace(f' ({p})', '')

        # Palavras‚Äëchave
        stop = {'de','da','do','das','dos','para','por','com','sem','uma','um','o','a','os','as','e','ou','no','na','nos','nas','em','ao','aos','√†','√†s','que','sobre','ap√≥s','contra','entre','como','mais','menos','hoje'}
        text_ref = f"{clean} {description}".lower()
        words = [w.strip(',.:;!?"()') for w in text_ref.split()]
        keys = []
        seen = set()
        for w in words:
            if len(w) > 2 and w not in stop and w not in seen:
                seen.add(w); keys.append(w)
        keys = keys[:6]

        base_topic = topic.title().strip() or 'Not√≠cia'
        
        # Estruturas de t√≠tulo mais naturais e variadas
        estruturas = []
        
        # Estrutura 1: Declara√ß√£o direta (sem dois pontos)
        if any(word in text_ref for word in ['dividendos', 'juros', 'impostos']):
            estruturas.append(f"{base_topic} {keys[0].title()} ‚Äî valores e datas" if keys else None)
            estruturas.append(f"{base_topic} {keys[0].title()} para acionistas" if keys else None)
        
        # Estrutura 2: Com dois pontos (apenas para explica√ß√µes)
        if any(word in text_ref for word in ['acordo', 'parceria', 'medidas']):
            estruturas.append(f"{base_topic} {keys[0].title()}: entenda os detalhes" if keys else None)
            estruturas.append(f"{base_topic} {keys[0].title()}: o que muda" if keys else None)
        
        # Estrutura 3: Interrogativa (para engajamento)
        if any(word in text_ref for word in ['anuncia', 'divulga', 'confirma']):
            estruturas.append(f"O que {base_topic} anuncia sobre {keys[0].title()}?" if keys else None)
            estruturas.append(f"Como {base_topic} atua em {keys[0].title()}?" if keys else None)
        
        # Estrutura 4: Declara√ß√£o simples (mais natural)
        estruturas.append(f"{base_topic} {keys[0].title()}" if keys else None)
        estruturas.append(f"{base_topic} {keys[0].title()} hoje" if keys else None)
        
        # Estrutura 5: Com tra√ßo (mais elegante)
        estruturas.append(f"{base_topic} {keys[0].title()} ‚Äî an√°lise completa" if keys else None)
        estruturas.append(f"{base_topic} {keys[0].title()} ‚Äî impactos e pr√≥ximos passos" if keys else None)
        
        # Padr√µes originais como fallback
        patterns = [
            lambda ks: f"{base_topic}: {ks[0].title()} e {ks[1].title()} ‚Äî entenda" if len(ks) >= 2 else None,
            lambda ks: f"{base_topic} hoje: {ks[0].title()} em foco" if ks else None,
            lambda ks: f"{clean} ‚Äî impactos e pr√≥ximos passos"[:140],
        ]
        
        # Adicionar padr√µes originais √†s estruturas
        for pattern in patterns:
            estruturas.append(pattern(keys))

        def norm(s):
            return re.sub(r'\s+', ' ', s.lower()).strip()
        orig_n = norm(original)
        
        # Testar todas as estruturas
        for estrutura in estruturas:
            if not estrutura:
                continue
            if 20 <= len(estrutura) <= 140 and norm(estrutura) != orig_n:
                return estrutura
                
        fallback = f"{clean} ‚Äî an√°lise" if clean else f"{base_topic}: √öltimas Not√≠cias"
        return fallback[:140]

    def _generate_content_from_news_fallback(self, article):
        """Gera conte√∫do fallback baseado na not√≠cia espec√≠fica"""
        title = article.get('title', '')
        description = article.get('description', '')
        source = article.get('source', '')
        topic = article.get('topic', '')
        
        content = f"""<p class="dek">{description}</p>

<h2>{title}</h2>

<p>Esta not√≠cia tem ganhado destaque nos √∫ltimos dias e merece aten√ß√£o especial. {description}</p>

<h3>Desenvolvimentos Recentes</h3>

<p>Os fatos relacionados a esta not√≠cia indicam uma evolu√ß√£o significativa no cen√°rio atual. A situa√ß√£o tem sido acompanhada de perto por especialistas e analistas que estudam o impacto dessas transforma√ß√µes.</p>

<p>Segundo informa√ß√µes da {source}, os desenvolvimentos mais recentes mostram uma evolu√ß√£o positiva em diversos indicadores relacionados ao tema.</p>

<h3>An√°lise do Impacto</h3>

<p>Esta not√≠cia tem relev√¢ncia especial no contexto atual, onde as particularidades locais influenciam diretamente os resultados observados. O impacto pode ser sentido em diferentes setores da sociedade.</p>

<p>Os especialistas destacam que esta situa√ß√£o reflete tend√™ncias mais amplas observadas em outros contextos, mas apresenta caracter√≠sticas √∫nicas que merecem aten√ß√£o especial.</p>

<h3>Contexto Brasileiro</h3>

<p>No Brasil, esta not√≠cia tem implica√ß√µes espec√≠ficas que afetam diretamente a vida dos cidad√£os brasileiros. Desde as grandes metr√≥poles at√© as cidades do interior, √© poss√≠vel observar mudan√ßas significativas relacionadas a esta quest√£o.</p>

<p>As autoridades brasileiras t√™m acompanhado de perto os desenvolvimentos, buscando adaptar as pol√≠ticas p√∫blicas √†s novas realidades apresentadas por esta not√≠cia.</p>

<h3>Perspectivas Futuras</h3>

<p>As proje√ß√µes para os pr√≥ximos meses indicam que esta tend√™ncia deve se manter, com poss√≠veis desenvolvimentos que podem trazer benef√≠cios adicionais. Os analistas s√£o cautelosamente otimistas quanto ao futuro.</p>

<p>Os investimentos planejados para os pr√≥ximos anos devem acelerar ainda mais essa tend√™ncia positiva, criando novas oportunidades e consolidando avan√ßos importantes.</p>

<h3>Recomenda√ß√µes</h3>

<p>Com base na an√°lise apresentada, √© poss√≠vel identificar algumas recomenda√ß√µes importantes para o desenvolvimento futuro desta quest√£o. Essas recomenda√ß√µes s√£o fundamentadas em dados concretos e na experi√™ncia de especialistas.</p>

<p>O primeiro passo √© continuar acompanhando os desenvolvimentos, garantindo que as informa√ß√µes mais atualizadas sejam consideradas nas tomadas de decis√£o.</p>

<h3>Conclus√£o</h3>

<p>Esta not√≠cia sobre {topic.lower() if topic else 'o tema em quest√£o'} foi desenvolvida com base em informa√ß√µes atualizadas e an√°lises de especialistas da √°rea. O RadarBR continua acompanhando os desdobramentos desta not√≠cia e manter√° os leitores informados sobre novos desenvolvimentos relacionados ao tema.</p>

<p>O cen√°rio atual √© promissor e indica que estamos no caminho certo para compreender melhor esta quest√£o. A continuidade do acompanhamento e o engajamento de todos os setores ser√£o fundamentais para manter o ritmo de evolu√ß√£o observado.</p>

<p>Para mais informa√ß√µes sobre {topic.lower() if topic else 'este tema'} e outros assuntos relevantes para o Brasil, acompanhe nossas atualiza√ß√µes di√°rias e mantenha-se sempre bem informado sobre os temas que mais importam para o pa√≠s.</p>"""

        return content

    def _get_category_from_news(self, article, Categoria):
        """Categoriza baseado no site de origem, not√≠cia encontrada ou sistema inteligente"""
        # 1. PRIORIDADE M√ÅXIMA: Extrair categoria do site de origem
        try:
            from rb_ingestor.site_categorizer import SiteCategorizer
            site_categorizer = SiteCategorizer()
            
            # Escolher URL a usar: original se dispon√≠vel
            url_to_use = article.get('original_url') or article.get('url') or ''
            # Evitar analisar Google News
            if url_to_use and 'news.google.' not in url_to_use:
                # Tentar extrair categoria do site
                site_category = site_categorizer.categorize_article({'url': url_to_use})
            else:
                site_category = None
            
            if site_category:
                self.stdout.write(f"üåê Categoria do site: {site_category}")
                
                # Buscar categoria existente
                cat = Categoria.objects.filter(nome=site_category.title()).first()
                if cat:
                    self.stdout.write(f"‚úÖ Usando categoria existente: {site_category}")
                    return cat
                
                # Criar nova categoria se n√£o existir
                cat, created = Categoria.objects.get_or_create(
                    slug=slugify(site_category)[:140],
                    defaults={"nome": site_category.title()}
                )
                if created:
                    self.stdout.write(f"üÜï Nova categoria criada: {site_category}")
                else:
                    self.stdout.write(f"‚úÖ Categoria encontrada: {site_category}")
                return cat
            
        except Exception as e:
            self.stdout.write(f"‚ö† Erro no categorizador de site: {e}")
        
        # 2. FALLBACK: Tentar usar categoria da not√≠cia encontrada
        news_category = article.get('category', '').strip()
        if news_category:
            # Limpar e normalizar categoria da not√≠cia
            clean_category = news_category.title().strip()
            self.stdout.write(f"üì∞ Categoria da not√≠cia: {clean_category}")
            
            # Buscar categoria existente
            cat = Categoria.objects.filter(nome=clean_category).first()
            if cat:
                self.stdout.write(f"‚úÖ Usando categoria existente: {clean_category}")
                return cat
            
            # Criar nova categoria se n√£o existir
            cat, created = Categoria.objects.get_or_create(
                slug=slugify(clean_category)[:140],
                defaults={"nome": clean_category}
            )
            if created:
                self.stdout.write(f"üÜï Nova categoria criada: {clean_category}")
            else:
                self.stdout.write(f"‚úÖ Categoria encontrada: {clean_category}")
            return cat
        
        # 3. FALLBACK FINAL: Sistema inteligente de an√°lise de conte√∫do
        self.stdout.write("üß† Usando sistema inteligente de categoriza√ß√£o...")
        title = article.get('title', '')
        description = article.get('description', '')
        topic = article.get('topic', '')
        
        try:
            from rb_ingestor.smart_categorizer import SmartCategorizer
            categorizer = SmartCategorizer()
            
            # Categorizar baseado no conte√∫do completo
            category_name = categorizer.categorize_content(title, description, topic)
            confidence = categorizer.get_category_confidence(title, description, topic)
            
            self.stdout.write(f"üß† Categoria detectada: {category_name} (confian√ßa: {confidence:.2f})")
            
            # Buscar ou criar categoria
            cat = Categoria.objects.filter(nome=category_name.title()).first()
            if cat:
                return cat
            
            # Criar nova categoria se n√£o existir
            cat, created = Categoria.objects.get_or_create(
                slug=slugify(category_name)[:140],
                defaults={"nome": category_name.title()}
            )
            return cat
            
        except Exception as e:
            self.stdout.write(f"‚ö† Erro no categorizador inteligente: {e}")
            # Fallback final para sistema simples
            return self._get_category_fallback(article, Categoria)
    
    def _get_category_fallback(self, article, Categoria):
        """Fallback para categoriza√ß√£o simples"""
        title = article.get('title', '').lower()
        description = article.get('description', '').lower()
        topic = article.get('topic', '').lower()
        
        all_text = f"{title} {description} {topic}"
        
        # Mapeamento simples de fallback
        if any(word in all_text for word in ['pris√£o','preso','pf','pol√≠cia','delegado','crime','criminoso','latroc√≠nio','assalto','tr√°fico','trafico','lavagem','pcc','mil√≠cia','milicia','tiroteio','homic√≠dio','homicidio']):
            category_name = "Brasil"
        elif any(word in all_text for word in ['pol√≠tica', 'governo', 'presidente', 'elei√ß√µes']):
            category_name = "Pol√≠tica"
        elif any(word in all_text for word in ['economia', 'mercado', 'infla√ß√£o', 'd√≥lar']):
            category_name = "Economia"
        elif any(word in all_text for word in ['esportes', 'futebol', 'copa']):
            category_name = "Esportes"
        elif any(word in all_text for word in ['sa√∫de', 'medicina', 'hospital']):
            category_name = "Sa√∫de"
        elif any(word in all_text for word in ['tecnologia', 'digital', 'ia']):
            category_name = "Tecnologia"
        else:
            category_name = "Brasil"
        
        cat = Categoria.objects.filter(nome=category_name).first()
        if cat:
            return cat
        
        cat, created = Categoria.objects.get_or_create(
            slug=slugify(category_name)[:140],
            defaults={"nome": category_name}
        )
        return cat

    def _add_specific_image(self, noticia, article):
        """Adiciona imagem apenas de bancos gratuitos (sem Instagram)."""
        try:
            self.stdout.write("üñºÔ∏è Buscando imagem em bancos gratuitos (Unsplash/Pexels)...")
            from rb_ingestor.image_search import ImageSearchEngine

            search_engine = ImageSearchEngine()

            topic = (article.get('topic', '') if article else '')
            title = (article.get('title', '') if article else '')

            search_term = topic if topic else title[:50]

            image_url = search_engine.search_image(
                search_term,
                noticia.conteudo,
                (noticia.categoria.nome if noticia.categoria else "geral")
            )

            if image_url:
                noticia.imagem = image_url
                noticia.imagem_alt = f"Imagem relacionada a {search_term}"
                noticia.imagem_credito = "Imagem gratuita"
                noticia.imagem_licenca = "CC"
                noticia.imagem_fonte_url = image_url
                noticia.save()

                self.stdout.write(f"‚úÖ Imagem gratuita adicionada: {search_term}")
                return

            self.stdout.write("‚ö†Ô∏è Nenhuma imagem encontrada nos bancos gratuitos")

        except Exception as e:
            self.stdout.write(f"‚ö†Ô∏è Erro ao adicionar imagem: {e}")

    def _check_duplicate(self, title, Noticia):
        """Verifica se j√° existe not√≠cia similar"""
        return Noticia.objects.filter(
            titulo__icontains=title[:20],
            criado_em__date=timezone.localdate()
        ).exists()

    def _ping_sitemap(self):
        """Faz ping do sitemap"""
        try:
            from core.utils import absolute_sitemap_url
            from rb_ingestor.ping import ping_search_engines
            
            sm_url = absolute_sitemap_url()
            res = ping_search_engines(sm_url)
            
            self.stdout.write(f"üîó Ping sitemap: Google={'OK' if res['google'] else 'NOK'}; Bing={'OK' if res['bing'] else 'NOK'}")
            
        except Exception as e:
            self.stdout.write(f"‚ö† Erro ao fazer ping do sitemap: {e}")
