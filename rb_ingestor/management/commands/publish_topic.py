# rb_ingestor/management/commands/publish_topic.py
"""
Comando completo para publicar artigo com t√≥pico especificado manualmente
Seguindo toda a l√≥gica do sistema RadarBR
"""
from django.core.management.base import BaseCommand
from django.utils import timezone
from django.utils.html import strip_tags
from slugify import slugify
from django.apps import apps
import logging
import random

logger = logging.getLogger(__name__)

class Command(BaseCommand):
    help = "Publica artigo com t√≥pico especificado manualmente seguindo toda a l√≥gica do sistema"

    def add_arguments(self, parser):
        parser.add_argument("topic", type=str, help="T√≥pico para o artigo")
        parser.add_argument("--category", type=str, help="Categoria espec√≠fica (opcional)")
        parser.add_argument("--title", type=str, help="T√≠tulo personalizado (opcional)")
        parser.add_argument("--force", action="store_true", help="For√ßa publica√ß√£o mesmo com duplicatas")
        parser.add_argument("--debug", action="store_true", help="Modo debug")
        parser.add_argument("--dry-run", action="store_true", help="Apenas simula, n√£o publica")
        parser.add_argument("--words", type=int, default=800, help="N√∫mero m√≠nimo de palavras (padr√£o: 800)")

    def handle(self, *args, **options):
        Noticia = apps.get_model("rb_noticias", "Noticia")
        Categoria = apps.get_model("rb_noticias", "Categoria")

        self.stdout.write("=== PUBLICA√á√ÉO MANUAL DE T√ìPICO ===")
        self.stdout.write(f"Executado em: {timezone.now()}")
        
        topic = options["topic"]
        category = options.get("category")
        custom_title = options.get("title")
        min_words = options["words"]
        
        self.stdout.write(f"Topico: {topic}")
        if category:
            self.stdout.write(f"Categoria especificada: {category}")
        if custom_title:
            self.stdout.write(f"Titulo personalizado: {custom_title}")
        self.stdout.write(f"Minimo de palavras: {min_words}")

        # Verificar duplicatas se n√£o for√ßar
        if not options["force"] and not options["dry_run"]:
            if self._check_duplicate(topic, Noticia):
                self.stdout.write("AVISO: Topico similar ja existe. Use --force para publicar mesmo assim.")
                return

        # Buscar not√≠cias espec√≠ficas sobre o t√≥pico
        news_article = self._search_specific_news(topic)
        
        if news_article:
            self.stdout.write(f"Noticia encontrada: {news_article.get('title', '')[:50]}...")
            
            # Acessar sites originais mencionados na not√≠cia
            enhanced_data = self._extract_from_original_sites(news_article)
            if enhanced_data:
                self.stdout.write(f"‚úÖ Conte√∫do extra√≠do de site original: {enhanced_data.get('source_domain', 'N/A')}")
                # Usar dados reais extra√≠dos
                news_article.update(enhanced_data)
            else:
                self.stdout.write("‚ö† Usando dados b√°sicos do Google News")
                # Melhorar os dados com informa√ß√µes mais espec√≠ficas
                if news_article.get('title') and news_article.get('description'):
                    enhanced_context = {
                        'title': news_article.get('title', ''),
                        'description': news_article.get('description', ''),
                        'source': news_article.get('source', 'Google News'),
                        'url': news_article.get('url', ''),
                        'specific_news': True
                    }
                    news_article.update(enhanced_context)
        else:
            self.stdout.write("AVISO: Nenhuma noticia especifica encontrada para o topico - criando do zero")

        # Detectar categoria
        if category:
            cat = Categoria.objects.filter(nome=category).first()
            if not cat:
                cat = Categoria.objects.create(nome=category, slug=slugify(category)[:140])
        else:
            cat = self._detect_category_from_news(topic.lower(), news_article, Categoria)

        # Gerar t√≠tulo
        if custom_title:
            title = custom_title
        else:
            title = self._generate_title_from_news(topic, news_article)

        # Gerar conte√∫do
        content = self._generate_content_from_news(topic, news_article, cat, min_words)
        
        # Verificar contagem de palavras
        word_count = len(strip_tags(content).split())
        self.stdout.write(f"Palavras geradas: {word_count}")
        
        # CATEGORIZAR BASEADO NO CONTE√öDO GERADO (apenas se categoria original n√£o foi detectada com alta confian√ßa)
        if not hasattr(self, '_original_category_confidence') or self._original_category_confidence < 0.6:
            self.stdout.write("üîç Analisando conte√∫do gerado para determinar categoria...")
            final_category = self._categorize_generated_content(content, topic)
            
            if final_category and final_category != cat:
                self.stdout.write(f"‚úÖ Categoria ajustada: {cat.nome} ‚Üí {final_category.nome}")
                cat = final_category
        else:
            self.stdout.write(f"‚úÖ Mantendo categoria original detectada: {cat.nome} (confian√ßa alta)")
        
        if word_count < min_words * 0.85:  # 85% do m√≠nimo
            self.stdout.write(f"AVISO: Conteudo com {word_count} palavras (minimo: {int(min_words * 0.85)}), ajustando...")
            content = self._adjust_content_length(content, topic, cat, min_words)
            word_count = len(strip_tags(content).split())
            self.stdout.write(f"Palavras apos ajuste: {word_count}")

        # Integrar v√≠deos do YouTube automaticamente
        try:
            from rb_ingestor.youtube_integration import YouTubeIntegration
            youtube_integration = YouTubeIntegration()
            
            content_with_video = youtube_integration.integrate_video_into_content(
                content, topic, title, news_article
            )
            
            if content_with_video != content:
                self.stdout.write("Video do YouTube integrado automaticamente")
                content = content_with_video
                word_count = len(strip_tags(content).split())
                self.stdout.write(f"Palavras apos integracao de video: {word_count}")
            
        except Exception as e:
            self.stdout.write(f"AVISO: Erro na integracao do YouTube: {e}")

        # Verificar se est√° dentro da margem ideal
        if min_words * 0.85 <= word_count <= min_words * 1.15:
            self.stdout.write(f"Conteudo dentro da margem ideal: {word_count} palavras")
        else:
            self.stdout.write(f"AVISO: Conteudo fora da margem ideal: {word_count} palavras")

        if options["dry_run"]:
            self.stdout.write("MODO DRY-RUN: Artigo nao sera publicado")
            self.stdout.write(f"Titulo: {title}")
            self.stdout.write(f"Categoria: {cat.nome if cat else 'N/A'}")
            self.stdout.write(f"Palavras: {word_count}")
            return

        # Criar not√≠cia
        noticia = Noticia.objects.create(
            titulo=title,
            conteudo=content,
            categoria=cat,
            slug=f"{slugify(title)[:120]}-{timezone.now().strftime('%Y%m%d%H%M%S')}",
            status=Noticia.Status.PUBLICADO,
            publicado_em=timezone.now(),
            fonte_url=f"manual-{topic.lower().replace(' ', '-')}-{timezone.now().strftime('%Y%m%d%H%M%S')}"
        )

        # Adicionar imagem
        self._add_image(noticia, topic, news_article)

        # Ping sitemap
        self._ping_sitemap()

        self.stdout.write("Artigo publicado com sucesso!")
        self.stdout.write(f"Titulo: {title}")
        self.stdout.write(f"Categoria: {cat.nome}")
        self.stdout.write(f"URL: {noticia.get_absolute_url()}")
        self.stdout.write(f"Palavras: {word_count}")
        self.stdout.write(f"Caracteres: {len(strip_tags(content))}")

    def _check_duplicate(self, topic, Noticia):
        """Verifica se j√° existe artigo similar"""
        topic_words = topic.lower().split()
        
        for word in topic_words:
            if len(word) > 3:
                similar = Noticia.objects.filter(titulo__icontains=word).first()
                if similar:
                    return True
        return False

    def _search_specific_news(self, topic):
        """Busca not√≠cias espec√≠ficas sobre o t√≥pico"""
        try:
            from gnews import GNews
            
            google_news = GNews()
            google_news.language = "pt"
            google_news.country = "BR"
            google_news.max_results = 1
            
            articles = google_news.get_news(topic)
            
            if articles:
                article = articles[0]
                # Verificar se o t√≥pico aparece no t√≠tulo ou descri√ß√£o
                if self._is_relevant_article(topic, article):
                    return article
            
            return None
            
        except Exception as e:
            self.stdout.write(f"AVISO: Erro ao buscar noticias: {e}")
            return None

    def _extract_from_original_sites(self, news_article):
        """Extrai conte√∫do do primeiro site que o Google News retornou"""
        try:
            # L√ìGICA SIMPLES: Acessar diretamente o primeiro resultado do Google News
            google_news_url = news_article.get('url', '')
            
            if google_news_url and 'news.google.com' in google_news_url:
                self.stdout.write("üîç Acessando primeiro resultado do Google News...")
                
                # Extrair URL original do Google News
                original_url = self._extract_original_url_from_google_news(google_news_url)
                
                if original_url:
                    self.stdout.write(f"‚úÖ URL original encontrado: {original_url}")
                    
                    # Acessar diretamente o artigo original
                    content = self._extract_content_from_url(original_url)
                    
                    if content and content.get('content') and len(content.get('content', '')) > 200:
                        self.stdout.write(f"‚úÖ Conte√∫do extra√≠do do artigo original")
                        return {
                            'title': content.get('title', ''),
                            'description': content.get('description', ''),
                            'content': content.get('content', ''),
                            'author': content.get('author', ''),
                            'date': content.get('date', ''),
                            'images': content.get('images', []),
                            'source_domain': self._extract_domain_from_url(original_url),
                            'original_url': original_url,
                            'real_content': True
                        }
                    else:
                        self.stdout.write("‚ö† Conte√∫do extra√≠do insuficiente, usando dados do Google News")
                else:
                    self.stdout.write("‚ö† N√£o foi poss√≠vel extrair URL original")
            
            # FALLBACK: Usar dados do Google News diretamente
            self.stdout.write("üîÑ Usando dados do Google News como base")
            return {
                'title': news_article.get('title', ''),
                'description': news_article.get('description', ''),
                'content': news_article.get('description', ''),  # Usar descri√ß√£o como conte√∫do
                'author': '',
                'date': news_article.get('published', ''),
                'images': [],
                'source_domain': 'google_news',
                'original_url': news_article.get('url', ''),
                'real_content': True
            }
            
        except Exception as e:
            self.stdout.write(f"AVISO: Erro ao extrair de sites originais: {e}")
            # FALLBACK FINAL: Usar dados do Google News
            return {
                'title': news_article.get('title', ''),
                'description': news_article.get('description', ''),
                'content': news_article.get('description', ''),
                'author': '',
                'date': news_article.get('published', ''),
                'images': [],
                'source_domain': 'google_news',
                'original_url': news_article.get('url', ''),
                'real_content': True
            }
    
    def _extract_original_url_from_google_news(self, google_news_url):
        """Extrai URL do ve√≠culo original a partir de um link do Google News.

        Estrat√©gia em camadas:
        1) Se houver par√¢metro "url" na query, retorna esse valor.
        2) Faz GET com allow_redirects=True e usa response.url (normalmente resolve para o site original).
        3) Fallback para heur√≠sticas em HTML/JS se o 1 e 2 falharem.
        """
        try:
            import requests
            from bs4 import BeautifulSoup
            import re
            from urllib.parse import urlparse, parse_qs, urljoin
            
            session = requests.Session()
            session.headers.update({
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            })

            # 1) Tentar extrair via querystring "url"
            try:
                parsed = urlparse(google_news_url)
                qs = parse_qs(parsed.query)
                if 'url' in qs and qs['url']:
                    candidate = qs['url'][0]
                    if candidate and not self._is_google_news_url(candidate):
                        return candidate
            except Exception:
                pass

            # 2) Seguir redirecionamentos
            self.stdout.write(f"üîç Acessando Google News: {google_news_url}")
            response = session.get(google_news_url, timeout=20, allow_redirects=True)
            response.raise_for_status()
            final_url = response.url
            if final_url and not self._is_google_news_url(final_url) and self._looks_like_news_url(final_url):
                self.stdout.write(f"üîó Resolvido por redirect: {final_url}")
                return final_url
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # M√âTODO 1: Procurar por links diretos que n√£o sejam do Google News
            all_links = soup.find_all('a', href=True)
            original_links = []
            
            for link in all_links:
                href = link.get('href')
                if href:
                    # Converter URLs relativos em absolutos
                    if href.startswith('/'):
                        href = urljoin(google_news_url, href)
                    
                    # Verificar se n√£o √© do Google News e parece ser uma not√≠cia
                    if not self._is_google_news_url(href) and self._looks_like_news_url(href):
                        original_links.append(href)
                        self.stdout.write(f"üîó Link encontrado: {href}")
            
            # M√âTODO 2: Procurar por meta tags Open Graph
            meta_tags = soup.find_all('meta')
            for meta in meta_tags:
                if meta.get('property') == 'og:url':
                    url = meta.get('content')
                    if url and not self._is_google_news_url(url) and self._looks_like_news_url(url):
                        original_links.append(url)
                        self.stdout.write(f"üîó Meta og:url encontrado: {url}")
            
            # M√âTODO 3: Procurar por JavaScript que pode conter URLs
            scripts = soup.find_all('script')
            for script in scripts:
                if script.string:
                    # Procurar por padr√µes de URL em JavaScript
                    url_patterns = [
                        r'https?://[^\s"\'<>]+\.(com|br|org|net)/[^\s"\'<>]*',
                        r'url["\']?\s*:\s*["\']([^"\']+)["\']',
                        r'href["\']?\s*:\s*["\']([^"\']+)["\']'
                    ]
                    
                    for pattern in url_patterns:
                        matches = re.findall(pattern, script.string)
                        for match in matches:
                            if isinstance(match, tuple):
                                url = match[0] if match[0] else match[1]
                            else:
                                url = match
                            
                            if url and not self._is_google_news_url(url) and self._looks_like_news_url(url):
                                original_links.append(url)
                                self.stdout.write(f"üîó URL encontrado em JS: {url}")
            
            # M√âTODO 4: Procurar por atributos data-* que podem conter URLs
            try:
                elements_with_data = soup.find_all(attrs=lambda attrs: attrs and hasattr(attrs, 'keys') and any(k.startswith('data-') for k in attrs.keys()))
                for element in elements_with_data:
                    if hasattr(element, 'attrs') and element.attrs:
                        for attr_name, attr_value in element.attrs.items():
                            if attr_name.startswith('data-') and isinstance(attr_value, str):
                                if 'http' in attr_value and not self._is_google_news_url(attr_value):
                                    if self._looks_like_news_url(attr_value):
                                        original_links.append(attr_value)
                                        self.stdout.write(f"üîó URL encontrado em data-*: {attr_value}")
            except Exception as e:
                self.stdout.write(f"‚ö† Erro ao processar atributos data-*: {e}")
            
            # Remover duplicatas e retornar o primeiro URL v√°lido
            unique_links = list(dict.fromkeys(original_links))
            
            for url in unique_links:
                if self._is_valid_news_url(url):
                    self.stdout.write(f"‚úÖ URL original v√°lido encontrado: {url}")
                    return url
            
            self.stdout.write("‚ùå Nenhum URL original v√°lido encontrado")
            return None
            
        except Exception as e:
            self.stdout.write(f"‚ö† Erro ao extrair URL original: {e}")
            return None
    
    def _is_google_news_url(self, url):
        """Verifica se o URL √© do Google News"""
        if not url:
            return False
        
        google_news_domains = [
            'news.google.com',
            'news.google.com.br',
            'news.google.co.uk'
        ]
        
        from urllib.parse import urlparse
        parsed = urlparse(url)
        return parsed.netloc.lower() in google_news_domains
    
    def _is_valid_news_url(self, url):
        """Verifica se o URL √© v√°lido para uma not√≠cia"""
        if not url:
            return False
        
        # URLs muito curtos provavelmente n√£o s√£o not√≠cias
        if len(url) < 20:
            return False
        
        # Verificar se n√£o √© um URL do Google News
        if self._is_google_news_url(url):
            return False
        
        # Verificar se parece ser um URL de not√≠cia
        return self._looks_like_news_url(url)
    
    def _extract_domain_from_url(self, url):
        """Extrai dom√≠nio de uma URL"""
        from urllib.parse import urlparse
        parsed = urlparse(url)
        return parsed.netloc.lower()
    
    def _find_specific_news_url(self, site_url, search_term):
        """Encontra URL espec√≠fico da not√≠cia no site"""
        try:
            import requests
            from bs4 import BeautifulSoup
            import re
            
            session = requests.Session()
            session.headers.update({
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            })
            
            # Tentar diferentes URLs de busca no site
            search_urls = [
                f"{site_url}/busca?q={search_term}",
                f"{site_url}/search?q={search_term}",
                f"{site_url}/noticias?q={search_term}",
                f"{site_url}/?q={search_term}"
            ]
            
            for search_url in search_urls:
                try:
                    self.stdout.write(f"üîç Buscando em: {search_url}")
                    response = session.get(search_url, timeout=10)
                    response.raise_for_status()
                    
                    soup = BeautifulSoup(response.content, 'html.parser')
                    
                    # Procurar por links de not√≠cias
                    news_links = []
                    
                    # Procurar por links que contenham palavras-chave da busca
                    search_words = search_term.lower().split()
                    
                    all_links = soup.find_all('a', href=True)
                    for link in all_links:
                        href = link.get('href')
                        link_text = link.get_text().lower()
                        
                        if href and any(word in link_text for word in search_words):
                            # Converter URL relativo em absoluto
                            if href.startswith('/'):
                                from urllib.parse import urljoin
                                href = urljoin(site_url, href)
                            
                            # Verificar se parece ser um link de not√≠cia
                            if self._looks_like_news_url(href) and site_url in href:
                                news_links.append(href)
                                self.stdout.write(f"üîó Link de not√≠cia encontrado: {href}")
                    
                    # Retornar o primeiro link v√°lido encontrado
                    for link in news_links:
                        if self._is_valid_news_url(link):
                            return link
                    
                except Exception as e:
                    self.stdout.write(f"‚ö† Erro ao buscar em {search_url}: {e}")
                    continue
            
            return None
            
        except Exception as e:
            self.stdout.write(f"‚ö† Erro ao encontrar URL espec√≠fico: {e}")
            return None
    
    def _extract_content_from_url(self, url):
        """Extrai conte√∫do de uma URL espec√≠fica"""
        try:
            import requests
            from bs4 import BeautifulSoup
            
            session = requests.Session()
            session.headers.update({
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            })
            
            response = session.get(url, timeout=15)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Extrair dados da not√≠cia
            extracted_data = {
                'url': url,
                'title': self._extract_title_from_soup(soup),
                'description': self._extract_description_from_soup(soup),
                'content': self._extract_content_from_soup(soup),
                'author': self._extract_author_from_soup(soup),
                'date': self._extract_date_from_soup(soup),
                'images': self._extract_images_from_soup(soup)
            }
            
            # Verificar se conseguiu extrair conte√∫do v√°lido
            if extracted_data['title'] and extracted_data['content'] and len(extracted_data['content']) > 200:
                return extracted_data
            
            return None
            
        except Exception as e:
            self.stdout.write(f"‚ö† Erro ao extrair conte√∫do de {url}: {e}")
            return None
    
    def _search_news_on_site(self, site_url, search_term):
        """Busca not√≠cias relacionadas em um site espec√≠fico"""
        try:
            import requests
            from bs4 import BeautifulSoup
            
            session = requests.Session()
            session.headers.update({
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            })
            
            # Tentar diferentes URLs de busca no site
            search_urls = [
                f"{site_url}/busca",
                f"{site_url}/search",
                f"{site_url}/noticias",
                f"{site_url}/ultimas-noticias",
                site_url
            ]
            
            for search_url in search_urls:
                try:
                    response = session.get(search_url, timeout=10)
                    if response.status_code == 200:
                        soup = BeautifulSoup(response.content, 'html.parser')
                        
                        # Procurar por links de not√≠cias
                        news_links = self._find_news_links(soup, site_url)
                        
                        if news_links:
                            # Tentar acessar a primeira not√≠cia encontrada
                            for news_url in news_links[:2]:
                                try:
                                    news_response = session.get(news_url, timeout=15)
                                    news_response.raise_for_status()
                                    
                                    news_soup = BeautifulSoup(news_response.content, 'html.parser')
                                    
                                    # Extrair dados da not√≠cia
                                    extracted_data = {
                                        'url': news_url,
                                        'title': self._extract_title_from_soup(news_soup),
                                        'description': self._extract_description_from_soup(news_soup),
                                        'content': self._extract_content_from_soup(news_soup),
                                        'author': self._extract_author_from_soup(news_soup),
                                        'date': self._extract_date_from_soup(news_soup),
                                        'images': self._extract_images_from_soup(news_soup)
                                    }
                                    
                                    # Verificar se conseguiu extrair conte√∫do v√°lido
                                    if extracted_data['title'] and extracted_data['content'] and len(extracted_data['content']) > 200:
                                        return extracted_data
                                    
                                except Exception as e:
                                    continue
                    
                except Exception as e:
                    continue
            
            return None
            
        except Exception as e:
            return None
    
    def _find_news_links(self, soup, site_url):
        """Encontra links de not√≠cias em uma p√°gina"""
        links = []
        
        # Procurar por links que parecem ser not√≠cias
        all_links = soup.find_all('a', href=True)
        
        for link in all_links:
            href = link.get('href')
            if href:
                # Converter URL relativa em absoluta
                if href.startswith('/'):
                    href = f"{site_url}{href}"
                elif href.startswith('//'):
                    href = f"https:{href}"
                
                # Verificar se parece ser um link de not√≠cia
                if self._looks_like_news_url(href):
                    links.append(href)
        
        return links[:5]  # Retornar at√© 5 links
    
    def _looks_like_news_url(self, url):
        """Verifica se o URL parece ser de uma not√≠cia"""
        if not url:
            return False
        
        # Padr√µes comuns de URLs de not√≠cias
        news_patterns = [
            r'/\d{4}/\d{2}/\d{2}/',  # Data no formato YYYY/MM/DD
            r'/noticia/',             # Cont√©m "noticia"
            r'/materia/',             # Cont√©m "materia"
            r'/artigo/',              # Cont√©m "artigo"
            r'\.html$',               # Termina com .html
            r'\.php$',                # Termina com .php
        ]
        
        import re
        for pattern in news_patterns:
            if re.search(pattern, url, re.IGNORECASE):
                return True
        
        return False
    
    def _extract_title_from_soup(self, soup):
        """Extrai t√≠tulo de um soup"""
        # Procurar por meta tags primeiro
        meta_title = soup.find('meta', property='og:title')
        if meta_title and meta_title.get('content'):
            return meta_title.get('content').strip()
        
        # Procurar por elementos de t√≠tulo
        title_selectors = ['h1', '.titulo', '.title', '.headline', '.noticia-titulo']
        for selector in title_selectors:
            element = soup.select_one(selector)
            if element:
                title = element.get_text().strip()
                if title and len(title) > 10:
                    return title
        return ''
    
    def _extract_description_from_soup(self, soup):
        """Extrai descri√ß√£o de um soup"""
        meta_desc = soup.find('meta', property='og:description')
        if meta_desc and meta_desc.get('content'):
            return meta_desc.get('content').strip()
        return ''
    
    def _extract_content_from_soup(self, soup):
        """Extrai conte√∫do principal de um soup"""
        # Remover elementos indesejados
        for element in soup(['script', 'style', 'nav', 'header', 'footer']):
            element.decompose()
        
        # Procurar por conte√∫do principal
        content_selectors = ['.conteudo', '.noticia-conteudo', '.materia-conteudo', '.texto', '.content', 'article']
        for selector in content_selectors:
            element = soup.select_one(selector)
            if element:
                content = element.get_text().strip()
                if content and len(content) > 100:
                    return content
        return ''
    
    def _extract_author_from_soup(self, soup):
        """Extrai autor de um soup"""
        author_selectors = ['.autor', '.author', '.byline']
        for selector in author_selectors:
            element = soup.select_one(selector)
            if element:
                return element.get_text().strip()
        return ''
    
    def _extract_date_from_soup(self, soup):
        """Extrai data de um soup"""
        date_selectors = ['.data', '.date', '.published', 'time']
        for selector in date_selectors:
            element = soup.select_one(selector)
            if element:
                return element.get_text().strip()
        return ''
    
    def _extract_images_from_soup(self, soup):
        """Extrai imagens de um soup"""
        images = []
        img_elements = soup.find_all('img')
        for img in img_elements[:3]:  # M√°ximo 3 imagens
            src = img.get('src') or img.get('data-src')
            if src:
                alt = img.get('alt', '')
                images.append({'src': src, 'alt': alt})
        return images

    def _is_relevant_article(self, topic, article):
        """Verifica se o artigo √© relevante para o t√≥pico"""
        title = article.get('title', '').lower()
        description = article.get('description', '').lower()
        
        # Verificar se o t√≥pico aparece no t√≠tulo ou descri√ß√£o
        topic_words = topic.lower().split()
        relevance_score = 0
        
        for word in topic_words:
            if len(word) > 3:  # Ignorar palavras muito curtas
                if word in title:
                    relevance_score += 2
                if word in description:
                    relevance_score += 1
        
        # Considerar relevante se score >= 2
        return relevance_score >= 2

    def _categorize_generated_content(self, content, topic):
        """Categoriza o artigo baseado no conte√∫do gerado"""
        try:
            from rb_ingestor.smart_categorizer import SmartCategorizer
            from rb_noticias.models import Categoria
            
            # Usar o SmartCategorizer para analisar o conte√∫do
            categorizer = SmartCategorizer()
            
            # Combinar t√≠tulo e conte√∫do para an√°lise
            text_to_analyze = f"{topic} {content}"
            
            # Categorizar usando o conte√∫do (retorna apenas o nome)
            category_name = categorizer.categorize_content("", text_to_analyze, topic)
            confidence = categorizer.get_category_confidence("", text_to_analyze, topic)
            self.stdout.write(f"üéØ Categoria detectada pelo conte√∫do: {category_name} (confian√ßa: {confidence:.2f})")
            
            # Buscar ou criar a categoria
            try:
                category = Categoria.objects.get(nome__iexact=category_name)
                self.stdout.write(f"‚úÖ Usando categoria existente: {category.nome}")
                return category
            except Categoria.DoesNotExist:
                # Criar nova categoria se n√£o existir
                category = Categoria.objects.create(nome=category_name.title())
                self.stdout.write(f"‚úÖ Nova categoria criada: {category.nome}")
                return category
                
        except Exception as e:
            self.stdout.write(f"‚ö† Erro na categoriza√ß√£o por conte√∫do: {e}")
            # Fallback para categoria padr√£o
            try:
                return Categoria.objects.get(nome__iexact='brasil')
            except Categoria.DoesNotExist:
                return Categoria.objects.first()
    
    def _generate_title_from_news(self, topic, news_article):
        """Gera t√≠tulo PR√ìPRIO SEO: Entidade + verbo + objeto: gancho (sem copiar)."""
        if not news_article:
            return f"{topic.title()}: √öltimas Not√≠cias"

        import re

        original = (news_article.get('title') or '').strip()
        description = (news_article.get('description') or '').strip()
        base_topic = topic.title().strip() or 'Not√≠cia'

        # Remover marcas de portal
        portals = ['G1','Globo','Folha','Estad√£o','UOL','Terra','R7','IG','Exame','Metr√≥poles','O Globo','CNN','BBC','Reuters']
        clean = original
        for p in portals:
            clean = clean.replace(f' - {p}', '').replace(f' | {p}', '').replace(f' ({p})', '')

        # Heur√≠sticas para entidade, verbo e objeto
        text_all = f"{clean}. {description}"
        # Entidade: primeira sequ√™ncia de palavras com inicial mai√∫scula
        m_ent = re.search(r'([A-Z√Å√â√ç√ì√ö√Ç√ä√î√É√ï][\w√Å√â√ç√ì√ö√Ç√ä√î√É√ï√ß√á√£√µ√¢√™√¥√≠√≥√∫√†√©√≠√≥√∫-]+(?:\s+[A-Z√Å√â√ç√ì√ö√Ç√ä√î√É√ï][\w√Å√â√ç√ì√ö√Ç√ä√î√É√ï√ß√á√£√µ√¢√™√¥√≠√≥√∫√†√©√≠√≥√∫-]+){0,2})', clean)
        entidade = (m_ent.group(1) if m_ent else base_topic).strip()
        # Verbo chave
        verbos_map = {
            'aprova':'aprova','anuncia':'anuncia','divulga':'divulga','entrega':'entrega','confirma':'confirma',
            'projeta':'projeta','corta':'corta','eleva':'eleva','recuar':'recua','recua':'recua','sobe':'sobe','cai':'cai'
        }
        verbo = None
        for v in verbos_map.keys():
            if re.search(rf'\b{v}\w*\b', text_all, re.IGNORECASE):
                verbo = verbos_map[v]; break
        verbo = verbo or 'anuncia'
        # Objeto
        objetos = ['dividendos','impostos','pre√ßos','tarifas','acordo','parceria','ref√©ns','san√ß√µes','investimentos','meta','juros']
        objeto = None
        for o in objetos:
            if re.search(rf'\b{o}\b', text_all, re.IGNORECASE):
                objeto = o; break
        objeto = objeto or (clean.split(':')[0].lower() if ':' in clean else base_topic.lower())

        # Estruturas de t√≠tulo mais naturais e variadas
        estruturas = []
        
        # Estrutura 1: Declara√ß√£o direta (sem dois pontos)
        if objeto in ['dividendos', 'juros', 'impostos']:
            estruturas.append(f"{entidade} {verbo} {objeto} ‚Äî valores e datas")
            estruturas.append(f"{entidade} {verbo} {objeto} para acionistas")
        
        # Estrutura 2: Com dois pontos (apenas para explica√ß√µes)
        if objeto in ['acordo', 'parceria', 'medidas']:
            estruturas.append(f"{entidade} {verbo} {objeto}: entenda os detalhes")
            estruturas.append(f"{entidade} {verbo} {objeto}: o que muda")
        
        # Estrutura 3: Interrogativa (para engajamento)
        if verbo in ['anuncia', 'divulga', 'confirma']:
            estruturas.append(f"O que {entidade} {verbo} sobre {objeto}?")
            estruturas.append(f"Como {entidade} {verbo} {objeto}?")
        
        # Estrutura 4: Declara√ß√£o simples (mais natural)
        estruturas.append(f"{entidade} {verbo} {objeto}")
        estruturas.append(f"{entidade} {verbo} {objeto} hoje")
        
        # Estrutura 5: Com tra√ßo (mais elegante)
        estruturas.append(f"{entidade} {verbo} {objeto} ‚Äî an√°lise completa")
        estruturas.append(f"{entidade} {verbo} {objeto} ‚Äî impactos e pr√≥ximos passos")

        # Remover duplicatas na entidade (ex.: "Petrobras Dividendos: Petrobras ...")
        if entidade.lower() in base_topic.lower():
            entidade = base_topic

        # Escolher a melhor estrutura baseada no contexto
        for estrutura in estruturas:
            # Substituir entidade na estrutura escolhida
            titulo = estrutura.replace("{entidade}", entidade).replace("{verbo}", verbo).replace("{objeto}", objeto)
            
            # Verificar se n√£o √© muito similar ao original
            if self._is_title_different_enough(titulo, clean):
                # Normalizar espa√ßos e limitar tamanho
                titulo = re.sub(r'\s+', ' ', titulo).strip()
                if 20 <= len(titulo) <= 140:
                    return titulo
        
        # Fallback: estrutura simples
        titulo = f"{entidade} {verbo} {objeto}"
        titulo = re.sub(r'\s+', ' ', titulo).strip()
        return titulo[:140]

    def _is_title_different_enough(self, new_title, original_title):
        """Verifica se o novo t√≠tulo √© suficientemente diferente do original"""
        import re
        
        if not original_title:
            return True
        
        # Normalizar ambos os t√≠tulos
        new_norm = re.sub(r'\s+', ' ', new_title.lower().strip())
        orig_norm = re.sub(r'\s+', ' ', original_title.lower().strip())
        
        # Se s√£o id√™nticos, n√£o usar
        if new_norm == orig_norm:
            return False
        
        # Se o novo t√≠tulo cont√©m mais de 80% das palavras do original, evitar
        new_words = set(new_norm.split())
        orig_words = set(orig_norm.split())
        
        if len(orig_words) > 0:
            similarity = len(new_words.intersection(orig_words)) / len(orig_words)
            if similarity > 0.8:
                return False
        
        return True

    def _generate_content_from_news(self, topic, news_article, category, min_words):
        """Gera conte√∫do baseado na not√≠cia espec√≠fica encontrada - MESMA L√ìGICA DA AUTOMA√á√ÉO"""
        try:
            # Usar sistema de IA melhorado (MESMO DA AUTOMA√á√ÉO)
            from rb_ingestor.ai_enhanced import generate_enhanced_article
            
            ai_content = generate_enhanced_article(topic, news_article, min_words)
            
            if ai_content:
                # Verificar qualidade do conte√∫do
                word_count = ai_content.get("word_count", 0)
                quality_score = ai_content.get("quality_score", 0)
                
                if word_count >= min_words and quality_score >= 40:  # Aceitar qualidade 40%+
                    self.stdout.write(f"IA melhorada gerou {word_count} palavras (qualidade: {quality_score}%)")
                    
                    # Usar o conte√∫do da IA diretamente
                    title = strip_tags(ai_content.get("title", topic.title()))[:200]
                    dek = strip_tags(ai_content.get("dek", news_article.get("description", "") if news_article else ""))[:220]
                    html_content = ai_content.get("html", "<p></p>")
                    
                    # Montar conte√∫do final
                    content = f'<p class="dek">{dek}</p>\n{html_content}'
                    return content
                else:
                    self.stdout.write(f"AVISO: IA gerou {word_count} palavras (qualidade: {quality_score}%), usando fallback")
            else:
                self.stdout.write("AVISO: IA n√£o retornou conte√∫do, usando fallback")
                
        except Exception as e:
            self.stdout.write(f"AVISO: Erro na IA melhorada: {e}")
        
        # FALLBACK MELHORADO: Usar a mesma l√≥gica da automa√ß√£o
        return self._generate_content_based_on_reference(topic, news_article, category, min_words)

    def _generate_content_from_news_fallback(self, topic, news_article, category, min_words):
        """Gera conte√∫do fallback baseado na not√≠cia espec√≠fica"""
        if not news_article:
            return self._generate_content_from_scratch(topic, category, min_words)
        
        # USAR APENAS A NOT√çCIA ESPEC√çFICA ENCONTRADA
        title = news_article.get("title", "")
        description = news_article.get("description", "")
        source = news_article.get("source", "")
        
        # Criar conte√∫do baseado EXCLUSIVAMENTE na not√≠cia encontrada
        content = f"""<p class="dek">{description}</p>

<h2>An√°lise da Not√≠cia</h2>

<p>Esta not√≠cia tem ganhado destaque e merece an√°lise detalhada. {title}</p>

<h3>Contexto e Desenvolvimentos</h3>

<p>Os fatos relacionados a esta not√≠cia indicam uma evolu√ß√£o significativa no cen√°rio atual. A situa√ß√£o tem sido acompanhada de perto por especialistas e analistas que estudam o impacto dessas transforma√ß√µes.</p>

<p>Segundo informa√ß√µes da {source}, os desenvolvimentos mais recentes mostram uma evolu√ß√£o positiva em diversos indicadores relacionados ao tema.</p>

<h3>An√°lise Detalhada</h3>

<p>Analisando os dados dispon√≠veis, √© poss√≠vel identificar padr√µes importantes que merecem aten√ß√£o. A not√≠cia sobre "{title}" representa um marco significativo no contexto atual.</p>

<p>Especialistas t√™m destacado a import√¢ncia deste desenvolvimento para o futuro do setor. As implica√ß√µes s√£o amplas e afetam diversos aspectos da sociedade.</p>

<h3>Impacto no Brasil</h3>

<p>No contexto brasileiro, esta not√≠cia tem repercuss√µes importantes. O pa√≠s tem acompanhado de perto os desenvolvimentos relacionados a este tema.</p>

<p>As autoridades brasileiras t√™m se posicionado de forma clara sobre o assunto, demonstrando preocupa√ß√£o com os impactos potenciais.</p>

<h3>Desenvolvimentos Recentes</h3>

<p>Os desenvolvimentos mais recentes relacionados a esta not√≠cia t√™m chamado a aten√ß√£o de especialistas e analistas. A evolu√ß√£o da situa√ß√£o tem sido acompanhada de perto por diversos setores da sociedade.</p>

<p>Segundo an√°lises realizadas por especialistas, os indicadores mostram uma tend√™ncia positiva que pode trazer benef√≠cios significativos para o pa√≠s.</p>

<h3>Perspectivas Futuras</h3>

<p>Olhando para o futuro, espera-se que novos desenvolvimentos surjam nos pr√≥ximos dias. A situa√ß√£o est√° em constante evolu√ß√£o.</p>

<p>Especialistas preveem que os pr√≥ximos passos ser√£o cruciais para determinar o rumo dos acontecimentos.</p>

<h3>Conclus√£o</h3>

<p>Esta not√≠cia representa um momento importante na evolu√ß√£o do tema. √â fundamental acompanhar os pr√≥ximos desenvolvimentos para entender completamente o impacto.</p>

<p>O RadarBR continuar√° acompanhando esta hist√≥ria e trar√° atualiza√ß√µes conforme novos fatos surjam.</p>"""

        return content

    def _generate_content_based_on_reference(self, topic, news_article, category, min_words):
        """Gera conte√∫do baseado em artigo de refer√™ncia com margem de +/-15% - MESMA L√ìGICA DA AUTOMA√á√ÉO"""
        try:
            # Usar IA melhorada com contexto espec√≠fico da not√≠cia
            from rb_ingestor.ai_enhanced import generate_enhanced_article
            
            # Calcular margem de palavras (¬±15%)
            target_words = min_words
            min_target = int(target_words * 0.85)
            max_target = int(target_words * 1.15)
            
            ai_content = generate_enhanced_article(topic, news_article, target_words)
            
            if ai_content:
                word_count = ai_content.get("word_count", 0)
                quality_score = ai_content.get("quality_score", 0)
                
                if min_target <= word_count <= max_target and quality_score >= 60:
                    self.stdout.write(f"Conteudo baseado em referencia: {word_count} palavras (qualidade: {quality_score}%)")
                    title = strip_tags(ai_content.get("title", topic.title()))[:200]
                    content = f'<p class="dek">{strip_tags(ai_content.get("dek", news_article.get("description", "") if news_article else ""))[:220]}</p>\n{ai_content.get("html", "<p></p>")}'
                    return content
                else:
                    self.stdout.write(f"AVISO: IA fora da margem ({word_count} palavras), ajustando...")
            
        except Exception as e:
            self.stdout.write(f"AVISO: Erro na IA melhorada: {e}")
        
        # Fallback: criar conte√∫do baseado na refer√™ncia
        return self._create_content_from_reference(topic, news_article, category, min_words)

    def _create_content_from_reference(self, topic, news_article, category, min_words):
        """Cria conte√∫do baseado na refer√™ncia encontrada"""
        if not news_article:
            return self._generate_content_from_scratch(topic, category, min_words)
        
        title = news_article.get("title", "")
        description = news_article.get("description", "")
        source = news_article.get("source", "")
        
        # Criar conte√∫do baseado na not√≠cia espec√≠fica
        content = f"""<p class="dek">{description}</p>

<h2>An√°lise da Not√≠cia</h2>

<p>Esta not√≠cia tem ganhado destaque e merece an√°lise detalhada. {title}</p>

<h3>Contexto e Desenvolvimentos</h3>

<p>Os fatos relacionados a esta not√≠cia indicam uma evolu√ß√£o significativa no cen√°rio atual. A situa√ß√£o tem sido acompanhada de perto por especialistas e analistas que estudam o impacto dessas transforma√ß√µes.</p>

<p>Segundo informa√ß√µes da {source}, os desenvolvimentos mais recentes mostram uma evolu√ß√£o positiva em diversos indicadores relacionados ao tema.</p>

<h3>An√°lise Detalhada</h3>

<p>Analisando os dados dispon√≠veis, √© poss√≠vel identificar padr√µes importantes que merecem aten√ß√£o. A not√≠cia sobre "{title}" representa um marco significativo no contexto atual.</p>

<p>Especialistas t√™m destacado a import√¢ncia deste desenvolvimento para o futuro do setor. As implica√ß√µes s√£o amplas e afetam diversos aspectos da sociedade.</p>

<h3>Impacto no Brasil</h3>

<p>No contexto brasileiro, esta not√≠cia tem repercuss√µes importantes. O pa√≠s tem acompanhado de perto os desenvolvimentos relacionados a este tema.</p>

<p>As autoridades brasileiras t√™m se posicionado de forma clara sobre o assunto, demonstrando preocupa√ß√£o com os impactos potenciais.</p>

<h3>Desenvolvimentos Recentes</h3>

<p>Os desenvolvimentos mais recentes relacionados a esta not√≠cia t√™m chamado a aten√ß√£o de especialistas e analistas. A evolu√ß√£o da situa√ß√£o tem sido acompanhada de perto por diversos setores da sociedade.</p>

<p>Segundo an√°lises realizadas por especialistas, os indicadores mostram uma tend√™ncia positiva que pode trazer benef√≠cios significativos para o pa√≠s.</p>

<h3>Perspectivas Futuras</h3>

<p>Olhando para o futuro, espera-se que novos desenvolvimentos surjam nos pr√≥ximos dias. A situa√ß√£o est√° em constante evolu√ß√£o.</p>

<p>Especialistas preveem que os pr√≥ximos passos ser√£o cruciais para determinar o rumo dos acontecimentos.</p>

<h3>Conclus√£o</h3>

<p>Esta not√≠cia representa um momento importante na evolu√ß√£o do tema. √â fundamental acompanhar os pr√≥ximos desenvolvimentos para entender completamente o impacto.</p>

<p>O RadarBR continuar√° acompanhando esta hist√≥ria e trar√° atualiza√ß√µes conforme novos fatos surjam.</p>"""

        return content

    def _generate_content_from_scratch(self, topic, category, min_words):
        """Gera conte√∫do do zero quando n√£o h√° not√≠cia espec√≠fica"""
        category_name = category.nome if category else "geral"
        
        content = f"""<p class="dek">An√°lise completa sobre {topic.lower()} e seus desenvolvimentos recentes no Brasil.</p>

<h2>Desenvolvimentos Recentes</h2>

<p>Esta not√≠cia tem ganhado destaque nos √∫ltimos dias e merece aten√ß√£o especial. {topic.title()} tem sido um tema de grande relev√¢ncia no cen√°rio atual.</p>

<h3>Contexto da Not√≠cia</h3>

<p>Os fatos relacionados a {topic.lower()} indicam uma evolu√ß√£o significativa no cen√°rio atual. A situa√ß√£o tem sido acompanhada de perto por especialistas e analistas que estudam o impacto dessas transforma√ß√µes.</p>

<p>Segundo informa√ß√µes de fontes especializadas, os desenvolvimentos mais recentes mostram uma evolu√ß√£o positiva em diversos indicadores relacionados ao tema.</p>

<h3>An√°lise Detalhada</h3>

<p>Analisando os dados dispon√≠veis, √© poss√≠vel identificar padr√µes importantes que merecem aten√ß√£o. {topic.title()} representa um marco significativo no contexto atual.</p>

<p>Especialistas t√™m destacado a import√¢ncia deste desenvolvimento para o futuro do setor. As implica√ß√µes s√£o amplas e afetam diversos aspectos da sociedade.</p>

<h3>Impacto no Brasil</h3>

<p>No contexto brasileiro, {topic.lower()} tem repercuss√µes importantes. O pa√≠s tem acompanhado de perto os desenvolvimentos relacionados a este tema.</p>

<p>As autoridades brasileiras t√™m se posicionado de forma clara sobre o assunto, demonstrando preocupa√ß√£o com os impactos potenciais.</p>

<h3>Desenvolvimentos Recentes</h3>

<p>Os desenvolvimentos mais recentes relacionados a {topic.lower()} t√™m chamado a aten√ß√£o de especialistas e analistas. A evolu√ß√£o da situa√ß√£o tem sido acompanhada de perto por diversos setores da sociedade.</p>

<p>Segundo an√°lises realizadas por especialistas, os indicadores mostram uma tend√™ncia positiva que pode trazer benef√≠cios significativos para o pa√≠s.</p>

<h3>Perspectivas Futuras</h3>

<p>Olhando para o futuro, espera-se que novos desenvolvimentos surjam nos pr√≥ximos dias. A situa√ß√£o est√° em constante evolu√ß√£o.</p>

<p>Especialistas preveem que os pr√≥ximos passos ser√£o cruciais para determinar o rumo dos acontecimentos.</p>

<h3>Conclus√£o</h3>

<p>{topic.title()} representa um momento importante na evolu√ß√£o do tema. √â fundamental acompanhar os pr√≥ximos desenvolvimentos para entender completamente o impacto.</p>

<p>O RadarBR continuar√° acompanhando esta hist√≥ria e trar√° atualiza√ß√µes conforme novos fatos surjam.</p>"""

        return content

    def _adjust_content_length(self, content, topic, category, min_words):
        """Ajusta o comprimento do conte√∫do para atingir o m√≠nimo de palavras"""
        current_words = len(strip_tags(content).split())
        
        if current_words >= min_words:
            return content
        
        # Adicionar se√ß√µes extras se necess√°rio
        additional_content = f"""

<h3>Desenvolvimento Nacional</h3>

<p>No contexto nacional, {topic.lower()} tem se mostrado um tema de grande relev√¢ncia para o desenvolvimento do Brasil. As iniciativas relacionadas a este assunto t√™m ganhado destaque.</p>

<p>O pa√≠s tem demonstrado capacidade de lideran√ßa nesta √°rea, com resultados positivos que beneficiam toda a sociedade.</p>

<h3>Impacto Regional no Brasil</h3>

<p>O impacto de {topic.lower()} varia significativamente entre as diferentes regi√µes do Brasil. No Nordeste, por exemplo, as caracter√≠sticas espec√≠ficas da regi√£o influenciam diretamente como este tema se desenvolve, criando oportunidades √∫nicas de crescimento e desenvolvimento.</p>

<p>Na regi√£o Sul, a tradi√ß√£o industrial e tecnol√≥gica oferece um ambiente prop√≠cio para o desenvolvimento de solu√ß√µes inovadoras relacionadas a {topic.lower()}. Esta vantagem competitiva tem sido aproveitada por empresas e profissionais locais.</p>

<h3>Tend√™ncias Emergentes</h3>

<p>As tend√™ncias emergentes relacionadas a {topic.lower()} indicam uma evolu√ß√£o constante e positiva. Novas tecnologias e metodologias est√£o sendo desenvolvidas, criando oportunidades para profissionais e empresas brasileiras.</p>

<p>Essas tend√™ncias s√£o acompanhadas de perto por especialistas e pesquisadores, que identificam padr√µes e desenvolvem estrat√©gias para aproveitar as oportunidades que surgem.</p>

<h3>Casos de Sucesso</h3>

<p>Existem diversos casos de sucesso relacionados a {topic.lower()} no Brasil que servem como refer√™ncia e inspira√ß√£o. Esses casos demonstram o potencial do pa√≠s e a capacidade dos profissionais brasileiros de desenvolver solu√ß√µes inovadoras.</p>

<p>Esses exemplos de sucesso s√£o fundamentais para motivar outros profissionais e empresas a investirem nesta √°rea, criando um ciclo virtuoso de crescimento e desenvolvimento.</p>

<h3>Recomenda√ß√µes e Pr√≥ximos Passos</h3>

<p>Com base na an√°lise apresentada, √© poss√≠vel identificar algumas recomenda√ß√µes importantes para o desenvolvimento futuro desta √°rea. Essas recomenda√ß√µes s√£o fundamentadas em dados concretos e na experi√™ncia de especialistas.</p>

<p>O primeiro passo √© continuar investindo em pesquisa e desenvolvimento, garantindo que o Brasil mantenha sua posi√ß√£o de lideran√ßa. Al√©m disso, √© importante focar na forma√ß√£o de profissionais qualificados.</p>"""

        return content + additional_content

    def _detect_category_from_news(self, topic_lower, news_article, Categoria):
        """Detecta categoria baseada no site de origem, not√≠cia encontrada ou sistema inteligente"""
        # 1. PRIORIDADE M√ÅXIMA: Extrair categoria do site de origem
        if news_article and news_article.get("original_url"):
            # Verificar se n√£o √© uma URL do Google News
            original_url = news_article.get("original_url")
            if self._is_google_news_url(original_url):
                self.stdout.write("‚ö† Pulando SiteCategorizer - URL √© do Google News")
            else:
                try:
                    from rb_ingestor.site_categorizer import SiteCategorizer
                    site_categorizer = SiteCategorizer()
                    
                    # Usar URL original em vez do Google News
                    original_news = {
                        'url': original_url,
                        'title': news_article.get('title', ''),
                        'description': news_article.get('description', '')
                    }
                    
                    # Tentar extrair categoria do site original
                    site_category = site_categorizer.categorize_article(original_news)
                    
                    if site_category:
                        self.stdout.write(f"Categoria do site: {site_category}")
                        
                        # Buscar categoria existente
                        cat = Categoria.objects.filter(nome=site_category.title()).first()
                        if cat:
                            self.stdout.write(f"Usando categoria existente: {site_category}")
                            return cat
                        
                        # Criar nova categoria se n√£o existir
                        cat, created = Categoria.objects.get_or_create(
                            slug=slugify(site_category)[:140],
                            defaults={"nome": site_category.title()}
                        )
                        if created:
                            self.stdout.write(f"Nova categoria criada: {site_category}")
                        else:
                            self.stdout.write(f"Categoria encontrada: {site_category}")
                        return cat
                    
                except Exception as e:
                    self.stdout.write(f"AVISO: Erro no categorizador de site: {e}")
        
        # 2. FALLBACK: Tentar usar categoria da not√≠cia encontrada
        if news_article and news_article.get("category"):
            news_category = news_article.get("category", "").strip()
            if news_category:
                # Limpar e normalizar categoria da not√≠cia
                clean_category = news_category.title().strip()
                self.stdout.write(f"Categoria da noticia: {clean_category}")
                
                # Buscar categoria existente
                cat = Categoria.objects.filter(nome=clean_category).first()
                if cat:
                    self.stdout.write(f"Usando categoria existente: {clean_category}")
                    return cat
                
                # Criar nova categoria se n√£o existir
                cat, created = Categoria.objects.get_or_create(
                    slug=slugify(clean_category)[:140],
                    defaults={"nome": clean_category}
                )
                if created:
                    self.stdout.write(f"Nova categoria criada: {clean_category}")
                else:
                    self.stdout.write(f"Categoria encontrada: {clean_category}")
                return cat
        
        # 3. FALLBACK FINAL: Sistema inteligente de an√°lise de conte√∫do
        self.stdout.write("Usando sistema inteligente de categorizacao...")
        title = news_article.get("title", "") if news_article else ""
        description = news_article.get("description", "") if news_article else ""
        
        try:
            from rb_ingestor.smart_categorizer import SmartCategorizer
            categorizer = SmartCategorizer()
            
            # Categorizar baseado no conte√∫do completo
            category_name = categorizer.categorize_content(title, description, topic_lower)
            confidence = categorizer.get_category_confidence(title, description, topic_lower)
            
            self.stdout.write(f"Categoria detectada: {category_name} (confianca: {confidence:.2f})")
            
            # Salvar confian√ßa da categoria original para uso posterior
            self._original_category_confidence = confidence
            
            # Buscar ou criar categoria
            cat = Categoria.objects.filter(nome=category_name.title()).first()
            if cat:
                return cat
            
            # Criar nova categoria se n√£o existir
            cat, created = Categoria.objects.get_or_create(
                slug=slugify(category_name)[:140],
                defaults={"nome": category_name.title()}
            )
            return cat
            
        except Exception as e:
            self.stdout.write(f"AVISO: Erro no categorizador inteligente: {e}")
            # Fallback final para sistema simples
            return self._get_category_fallback(topic_lower, Categoria)

    def _get_category_fallback(self, topic_lower, Categoria):
        """Fallback simples para categoriza√ß√£o"""
        category_mapping = {
            "politica": "Pol√≠tica",
            "economia": "Economia", 
            "esportes": "Esportes",
            "tecnologia": "Tecnologia",
            "saude": "Sa√∫de",
            "mundo": "Mundo",
            "brasil": "Brasil"
        }
        
        for keyword, category_name in category_mapping.items():
            if keyword in topic_lower:
                cat = Categoria.objects.filter(nome=category_name).first()
                if cat:
                    return cat
                return Categoria.objects.create(nome=category_name, slug=slugify(category_name)[:140])
        
        # Default
        cat = Categoria.objects.filter(nome="Brasil").first()
        if cat:
            return cat
        return Categoria.objects.create(nome="Brasil", slug="brasil")

    def _add_image(self, noticia, topic, news_article=None):
        """Adiciona imagem √† not√≠cia seguindo l√≥gica inteligente"""
        try:
            # L√ìGICA INTELIGENTE MELHORADA:
            # 1. Figuras p√∫blicas: Detec√ß√£o inteligente ‚Üí Rede social do artigo original ‚Üí Instagram oficial ‚Üí Bancos gratuitos
            # 2. Artigos gerais: Bancos gratuitos
            
            # NOVA PRIORIDADE 1: Detec√ß√£o inteligente de figuras p√∫blicas
            from rb_ingestor.smart_public_figure_detector import SmartPublicFigureDetector
            smart_detector = SmartPublicFigureDetector()
            
            full_text = f"{noticia.titulo} {noticia.conteudo}"
            if news_article:
                full_text += f" {news_article.get('title', '')} {news_article.get('description', '')}"
            
            # Detectar figura p√∫blica usando sistema inteligente
            public_figure = smart_detector.detect_public_figure(full_text)
            
            if public_figure:
                # √â figura p√∫blica - seguir l√≥gica espec√≠fica
                self.stdout.write(f"Figura publica detectada: {public_figure['figure']}")
                
                # PRIORIDADE 1: Imagem de rede social no artigo original
                if news_article:
                    from rb_ingestor.instagram_image_finder import InstagramImageFinder
                    instagram_finder = InstagramImageFinder()
                    social_image = instagram_finder.extract_social_media_images_from_news(news_article)
                    if social_image:
                        self.stdout.write(f"Imagem de rede social encontrada no artigo original")
                        noticia.imagem = social_image["url"]
                        noticia.imagem_alt = social_image.get("alt", f"Imagem de {public_figure['figure']}")
                        noticia.imagem_credito = social_image.get("credit", f"Foto: {public_figure['instagram_handle']}")
                        noticia.imagem_licenca = "Rede Social - Figura P√∫blica"
                        noticia.imagem_fonte_url = social_image.get("url", "")
                        noticia.save()
                        
                        self.stdout.write("Imagem de rede social adicionada com sucesso")
                        return
                
                # PRIORIDADE 2: Instagram oficial da figura (usando sistema inteligente)
                instagram_image = smart_detector.get_instagram_image_for_figure(public_figure)
                
                if instagram_image and instagram_image.get("url"):
                    self.stdout.write(f"Imagem do Instagram oficial encontrada: {public_figure['instagram_handle']}")
                    noticia.imagem = instagram_image["url"]
                    noticia.imagem_alt = instagram_image.get("alt", f"Imagem de {public_figure['figure']}")
                    noticia.imagem_credito = instagram_image.get("credit", f"Foto: Instagram {public_figure['instagram_handle']}")
                    noticia.imagem_licenca = "Figura P√∫blica - Unsplash"
                    noticia.imagem_fonte_url = instagram_image.get("instagram_url", "")
                    noticia.save()
                    
                    self.stdout.write("Imagem do Instagram oficial adicionada com sucesso")
                    return
            
            # FALLBACK: Bancos de imagens gratuitos (para artigos gerais ou quando Instagram n√£o funciona)
            self.stdout.write("Usando banco de imagens gratuitos...")
            from rb_ingestor.image_search import ImageSearchEngine
            search_engine = ImageSearchEngine()
            
            image_url = search_engine.search_image(
                noticia.titulo,
                noticia.conteudo,
                noticia.categoria.nome if noticia.categoria else "geral"
            )

            if image_url:
                noticia.imagem = image_url
                noticia.imagem_alt = f"Imagem relacionada a {topic}"
                noticia.imagem_credito = "Imagem gratuita"
                noticia.imagem_licenca = "CC"
                noticia.imagem_fonte_url = image_url
                noticia.save()

                self.stdout.write("Imagem gratuita adicionada com sucesso")
                return
            
            self.stdout.write("AVISO: Nenhuma imagem encontrada")

        except Exception as e:
            self.stdout.write(f"AVISO: Erro ao adicionar imagem: {e}")

    def _ping_sitemap(self):
        """Faz ping do sitemap"""
        try:
            from core.utils import absolute_sitemap_url
            from rb_ingestor.ping import ping_search_engines
            
            sm_url = absolute_sitemap_url()
            res = ping_search_engines(sm_url)
            
            self.stdout.write(f"Ping sitemap: Google={'OK' if res['google'] else 'NOK'}; Bing={'OK' if res['bing'] else 'NOK'}")
            
        except Exception as e:
            self.stdout.write(f"AVISO: Erro no ping do sitemap: {e}")