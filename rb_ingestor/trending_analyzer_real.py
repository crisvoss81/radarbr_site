# rb_ingestor/trending_analyzer_real.py
"""
Sistema de an√°lise de tend√™ncias REAIS com atualiza√ß√£o autom√°tica
"""
import requests
import json
from datetime import datetime, timedelta
from typing import List, Dict, Tuple
from django.utils import timezone
from django.core.cache import cache
import time

class RealTrendingAnalyzer:
    def __init__(self):
        self.google_trends_api = "https://trends.google.com/trends/api/dailytrends"
        self.reddit_api = "https://www.reddit.com/r/brasil/hot.json"
        self.twitter_api = "https://api.twitter.com/1.1/trends/place.json"
        self.youtube_api = "https://www.googleapis.com/youtube/v3/videos"
        
    def get_real_google_trends(self) -> List[Dict]:
        """Busca tend√™ncias REAIS do Google Trends Brasil"""
        try:
            # Tentar usar PyTrends se dispon√≠vel
            try:
                from pytrends.request import TrendReq
                
                pytrends = TrendReq(hl='pt-BR', tz=360, geo='BR')
                
                # Buscar trending searches reais
                trending_searches = pytrends.trending_searches(pn='brazil')
                
                topics = []
                if not trending_searches.empty:
                    for i, topic in enumerate(trending_searches.head(10)[0]):  # Corrigir acesso ao DataFrame
                        volume = self._estimate_search_volume(topic)
                        category = self._categorize_topic(topic)
                        
                        topics.append({
                            "topic": topic,
                            "search_volume": volume,
                            "category": category,
                            "source": "google_trends_real",
                            "timestamp": timezone.now().isoformat()
                        })
                
                return topics
                
            except ImportError:
                # Fallback para API direta do Google Trends
                return self._get_google_trends_api()
                
        except Exception as e:
            print(f"Erro ao buscar Google Trends real: {e}")
            # Retornar t√≥picos de fallback espec√≠ficos para Google Trends
            return self._get_google_trends_fallback()
    
    def _get_google_trends_api(self) -> List[Dict]:
        """Busca via API direta do Google Trends"""
        try:
            # URL para trending searches do Brasil
            url = "https://trends.google.com/trends/api/dailytrends"
            params = {
                'hl': 'pt-BR',
                'tz': '-180',  # UTC-3 (Brasil)
                'geo': 'BR',
                'ns': '15'
            }
            
            response = requests.get(url, params=params, timeout=10)
            if response.status_code == 200:
                # Parse do JSON (remove prefixo do Google)
                data = response.text[5:]  # Remove ")]}',"
                trends_data = json.loads(data)
                
                topics = []
                for trend in trends_data.get('default', {}).get('trendingSearchesDays', [{}])[0].get('trendingSearches', [])[:10]:
                    topic = trend.get('title', {}).get('query', '')
                    if topic:
                        topics.append({
                            "topic": topic,
                            "search_volume": self._estimate_search_volume(topic),
                            "category": self._categorize_topic(topic),
                            "source": "google_trends_api",
                            "timestamp": timezone.now().isoformat()
                        })
                
                return topics
                
        except Exception as e:
            print(f"Erro na API do Google Trends: {e}")
            
        return []
    
    def get_twitter_trends_br(self) -> List[Dict]:
        """Busca tend√™ncias do Twitter Brasil"""
        try:
            # Simula√ß√£o - API real requer autentica√ß√£o
            # Em produ√ß√£o, usar Twitter API v2
            twitter_trends = [
                {"topic": "Brasil", "volume": "high"},
                {"topic": "S√£o Paulo", "volume": "medium"},
                {"topic": "Rio de Janeiro", "volume": "medium"},
                {"topic": "Futebol", "volume": "high"},
                {"topic": "Pol√≠tica", "volume": "medium"}
            ]
            
            topics = []
            for trend in twitter_trends:
                topics.append({
                    "topic": trend["topic"],
                    "search_volume": trend["volume"],
                    "category": self._categorize_topic(trend["topic"]),
                    "source": "twitter",
                    "timestamp": timezone.now().isoformat()
                })
            
            return topics
            
        except Exception as e:
            print(f"Erro ao buscar Twitter trends: {e}")
            return []
    
    def get_youtube_trending_br(self) -> List[Dict]:
        """Busca v√≠deos em alta no YouTube Brasil"""
        try:
            # Simula√ß√£o - API real requer chave
            youtube_trends = [
                {"topic": "M√∫sica brasileira", "volume": "high"},
                {"topic": "Tutorial", "volume": "medium"},
                {"topic": "Entretenimento", "volume": "high"},
                {"topic": "Educa√ß√£o", "volume": "medium"},
                {"topic": "Tecnologia", "volume": "medium"}
            ]
            
            topics = []
            for trend in youtube_trends:
                topics.append({
                    "topic": trend["topic"],
                    "search_volume": trend["volume"],
                    "category": self._categorize_topic(trend["topic"]),
                    "source": "youtube",
                    "timestamp": timezone.now().isoformat()
                })
            
            return topics
            
        except Exception as e:
            print(f"Erro ao buscar YouTube trends: {e}")
            return []
    
    def get_reddit_trending(self) -> List[Dict]:
        """Busca posts populares do Reddit Brasil"""
        try:
            headers = {'User-Agent': 'RadarBR/1.0'}
            response = requests.get(self.reddit_api, headers=headers, timeout=10)
            
            if response.status_code == 200:
                data = response.json()
                topics = []
                
                for post in data.get('data', {}).get('children', [])[:10]:
                    post_data = post.get('data', {})
                    title = post_data.get('title', '')
                    
                    if title and len(title) > 10:
                        # Extrair t√≥pico do t√≠tulo
                        topic = self._extract_topic_from_title(title)
                        if topic:
                            topics.append({
                                "topic": topic,
                                "search_volume": "medium",
                                "category": self._categorize_topic(topic),
                                "source": "reddit",
                                "timestamp": timezone.now().isoformat(),
                                "score": post_data.get('score', 0)
                            })
                
                return topics
                
        except Exception as e:
            print(f"Erro ao buscar Reddit: {e}")
            
        return []
    
    def get_cached_trends(self, cache_hours: int = 2) -> List[Dict]:
        """Busca tend√™ncias com cache configur√°vel"""
        cache_key = f"real_trends_{timezone.now().strftime('%Y-%m-%d-%H')}"
        
        # Verificar cache
        cached_data = cache.get(cache_key)
        if cached_data:
            print(f"‚úÖ Usando tend√™ncias do cache: {len(cached_data)} t√≥picos")
            return cached_data
        
        # Buscar dados novos
        print("üîÑ Buscando tend√™ncias reais...")
        all_topics = []
        
        # Combinar m√∫ltiplas fontes
        all_topics.extend(self.get_real_google_trends())
        all_topics.extend(self.get_twitter_trends_br())
        all_topics.extend(self.get_youtube_trending_br())
        all_topics.extend(self.get_reddit_trending())
        
        # Remover duplicatas e ordenar
        unique_topics = self._deduplicate_and_rank(all_topics)
        
        # Salvar no cache
        cache.set(cache_key, unique_topics, cache_hours * 3600)
        
        print(f"‚úÖ Tend√™ncias atualizadas: {len(unique_topics)} t√≥picos √∫nicos")
        return unique_topics
    
    def _deduplicate_and_rank(self, topics: List[Dict]) -> List[Dict]:
        """Remove duplicatas e ordena por relev√¢ncia"""
        seen_topics = set()
        unique_topics = []
        
        for topic in topics:
            topic_key = topic["topic"].lower().strip()
            if topic_key not in seen_topics:
                seen_topics.add(topic_key)
                
                # Calcular score de relev√¢ncia
                topic["relevance_score"] = self._calculate_relevance_score(topic)
                unique_topics.append(topic)
        
        # Ordenar por score de relev√¢ncia
        unique_topics.sort(key=lambda x: x["relevance_score"], reverse=True)
        
        return unique_topics
    
    def _calculate_relevance_score(self, topic: Dict) -> float:
        """Calcula score de relev√¢ncia do t√≥pico"""
        score = 0.0
        
        # Score baseado na fonte
        source_scores = {
            "google_trends_real": 10.0,
            "google_trends_api": 9.0,
            "twitter": 8.0,
            "youtube": 7.0,
            "reddit": 6.0
        }
        score += source_scores.get(topic["source"], 5.0)
        
        # Score baseado no volume de busca
        volume_scores = {
            "high": 3.0,
            "medium": 2.0,
            "low": 1.0
        }
        score += volume_scores.get(topic["search_volume"], 1.0)
        
        # Score baseado na categoria
        category_scores = {
            "pol√≠tica": 2.0,
            "economia": 2.0,
            "tecnologia": 1.5,
            "esportes": 1.5,
            "sa√∫de": 1.0,
            "educa√ß√£o": 1.0
        }
        score += category_scores.get(topic["category"], 0.5)
        
        # Score baseado no Reddit (se dispon√≠vel)
        if "score" in topic and topic["score"] > 100:
            score += 1.0
        
        return score
    
    def _estimate_search_volume(self, topic: str) -> str:
        """Estima volume de busca baseado no t√≥pico"""
        topic_lower = topic.lower()
        
        # Palavras-chave de alto volume
        high_volume_keywords = [
            "brasil", "brasileiro", "governo", "presidente", "elei√ß√µes",
            "futebol", "copa", "mundial", "infla√ß√£o", "economia",
            "chatgpt", "ia", "intelig√™ncia artificial", "tecnologia"
        ]
        
        # Palavras-chave de m√©dio volume
        medium_volume_keywords = [
            "s√£o paulo", "rio de janeiro", "minas gerais", "bahia",
            "sa√∫de", "educa√ß√£o", "meio ambiente", "sustentabilidade"
        ]
        
        if any(kw in topic_lower for kw in high_volume_keywords):
            return "high"
        elif any(kw in topic_lower for kw in medium_volume_keywords):
            return "medium"
        else:
            return "low"
    
    def _categorize_topic(self, topic: str) -> str:
        """Categoriza t√≥pico automaticamente"""
        topic_lower = topic.lower()
        
        category_keywords = {
            "pol√≠tica": ["governo", "presidente", "elei√ß√µes", "congresso", "ministro", "lula", "bolsonaro"],
            "economia": ["infla√ß√£o", "economia", "mercado", "d√≥lar", "real", "pib", "emprego"],
            "tecnologia": ["chatgpt", "ia", "intelig√™ncia artificial", "tecnologia", "digital", "app"],
            "esportes": ["futebol", "copa", "mundial", "brasileir√£o", "flamengo", "palmeiras"],
            "sa√∫de": ["sa√∫de", "medicina", "hospital", "vacina", "covid", "coronav√≠rus"],
            "educa√ß√£o": ["educa√ß√£o", "escola", "universidade", "enem", "vestibular"],
            "meio ambiente": ["meio ambiente", "sustentabilidade", "natureza", "clima", "ecologia"],
            "entretenimento": ["m√∫sica", "cinema", "tv", "streaming", "netflix", "entretenimento"]
        }
        
        for category, keywords in category_keywords.items():
            if any(kw in topic_lower for kw in keywords):
                return category
        
        return "geral"
    
    def _extract_topic_from_title(self, title: str) -> str:
        """Extrai t√≥pico relevante do t√≠tulo"""
        common_words = [
            'no', 'do', 'da', 'em', 'para', 'com', 'por', 'que', '√©', 'foi',
            'ser', 'ter', 'h√°', 'mais', 'menos', 'sobre', 'a', 'o', 'as', 'os',
            'um', 'uma', 'de', 'e', 'ou', 'mas', 'se', 'n√£o', 'j√°', 'ainda'
        ]
        
        title_clean = title.lower()
        words = title_clean.split()
        
        relevant_words = [word for word in words if word not in common_words and len(word) > 3]
        
        if relevant_words:
            return ' '.join(relevant_words[:3])
        
        return None
    
    def _get_google_trends_fallback(self) -> List[Dict]:
        """Fallback espec√≠fico para Google Trends com t√≥picos atuais"""
        fallback_topics = [
            {"topic": "ChatGPT Brasil", "search_volume": "high", "category": "tecnologia"},
            {"topic": "Infla√ß√£o 2025", "search_volume": "high", "category": "economia"},
            {"topic": "Elei√ß√µes municipais", "search_volume": "medium", "category": "pol√≠tica"},
            {"topic": "Copa do Mundo 2026", "search_volume": "high", "category": "esportes"},
            {"topic": "Crise h√≠drica", "search_volume": "medium", "category": "meio ambiente"},
            {"topic": "Intelig√™ncia artificial", "search_volume": "high", "category": "tecnologia"},
            {"topic": "Economia brasileira", "search_volume": "medium", "category": "economia"},
            {"topic": "Pol√≠tica nacional", "search_volume": "high", "category": "pol√≠tica"}
        ]
        
        topics = []
        for topic in fallback_topics:
            topics.append({
                **topic,
                "source": "google_trends_fallback",
                "timestamp": timezone.now().isoformat(),
                "relevance_score": 7.0
            })
        
        return topics
    
    def _get_fallback_topics(self) -> List[Dict]:
        """T√≥picos de fallback quando APIs falham"""
        fallback_topics = [
            {"topic": "Brasil", "search_volume": "high", "category": "geral"},
            {"topic": "Economia brasileira", "search_volume": "medium", "category": "economia"},
            {"topic": "Tecnologia no Brasil", "search_volume": "medium", "category": "tecnologia"},
            {"topic": "Pol√≠tica nacional", "search_volume": "high", "category": "pol√≠tica"},
            {"topic": "Esportes brasileiros", "search_volume": "medium", "category": "esportes"}
        ]
        
        topics = []
        for topic in fallback_topics:
            topics.append({
                **topic,
                "source": "fallback",
                "timestamp": timezone.now().isoformat(),
                "relevance_score": 5.0
            })
        
        return topics
    
    def get_optimized_topics(self, limit: int = 5) -> List[Dict]:
        """Retorna t√≥picos otimizados com tend√™ncias reais"""
        # Usar cache de 2 horas
        all_topics = self.get_cached_trends(cache_hours=2)
        
        # Retornar os melhores
        return all_topics[:limit]
    
    def force_update_trends(self) -> List[Dict]:
        """For√ßa atualiza√ß√£o das tend√™ncias (ignora cache)"""
        cache_key = f"real_trends_{timezone.now().strftime('%Y-%m-%d-%H')}"
        cache.delete(cache_key)
        
        return self.get_cached_trends(cache_hours=2)