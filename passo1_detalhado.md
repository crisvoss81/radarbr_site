# PASSO 1: BUSCA E EXTRA√á√ÉO DE FONTE - AN√ÅLISE DETALHADA

## Fluxo Atual do Passo 1

```mermaid
flowchart TD
    A[Entrada: T√≥pico/Assunto] --> B[GNews.get_news\(t√≥pico\)]
    B --> C[Primeiro resultado do Google News]
    C --> D[Verifica relev√¢ncia do artigo]
    D --> E{Artigo relevante?}
    
    E -->|N√£o| F[Retorna None - Criar do zero]
    E -->|Sim| G[Extrai URL do Google News]
    
    G --> H[Estrat√©gia 1: Query Parameter 'url']
    H --> I{URL v√°lida encontrada?}
    I -->|Sim| J[Retorna URL original]
    I -->|N√£o| K[Estrat√©gia 2: Seguir redirecionamentos]
    
    K --> L[GET com allow_redirects=True]
    L --> M{URL final v√°lida?}
    M -->|Sim| N[Retorna URL resolvida]
    M -->|N√£o| O[Estrat√©gia 3: Scraping HTML]
    
    O --> P[Procurar links diretos]
    P --> Q[Procurar meta og:url]
    Q --> R[Procurar URLs em JavaScript]
    R --> S[Procurar atributos data-*]
    
    S --> T[Filtrar URLs v√°lidos]
    T --> U{URL v√°lido encontrado?}
    U -->|Sim| V[Retorna primeiro URL v√°lido]
    U -->|N√£o| W[Retorna None]
    
    J --> X[Acessar artigo original]
    N --> X
    V --> X
    W --> Y[Usar dados do Google News]
    
    X --> Z[Extrair conte√∫do do artigo]
    Z --> AA{Conte√∫do suficiente?}
    AA -->|Sim| BB[Retorna dados completos]
    AA -->|N√£o| CC[Fallback: Google News]
    
    Y --> CC
    CC --> DD[Dados b√°sicos: t√≠tulo + descri√ß√£o]
    
    BB --> EE[‚úÖ Artigo original como base]
    DD --> FF[‚ö†Ô∏è Dados limitados do Google News]
    
    style A fill:#e1f5fe
    style EE fill:#c8e6c9
    style FF fill:#ffecb3
    style F fill:#ffcdd2
```

## Detalhamento das Estrat√©gias

### 1. **Busca Inicial (GNews)**
```python
def _search_specific_news(self, topic):
    google_news = GNews()
    google_news.language = "pt"
    google_news.country = "BR"
    google_news.max_results = 1
    
    articles = google_news.get_news(topic)
    # Verifica se o t√≥pico aparece no t√≠tulo ou descri√ß√£o
    if self._is_relevant_article(topic, article):
        return article
```

### 2. **Extra√ß√£o de URL Original (4 Estrat√©gias)**

#### **Estrat√©gia 1: Query Parameter**
```python
# Extrai 'url' da query string do Google News
parsed = urlparse(google_news_url)
qs = parse_qs(parsed.query)
if 'url' in qs and qs['url']:
    candidate = qs['url'][0]
    if not self._is_google_news_url(candidate):
        return candidate
```

#### **Estrat√©gia 2: Redirecionamentos HTTP**
```python
# Segue redirecionamentos automaticamente
response = session.get(google_news_url, allow_redirects=True)
final_url = response.url
if not self._is_google_news_url(final_url):
    return final_url
```

#### **Estrat√©gia 3: Scraping HTML**
```python
# Procura links diretos no HTML
all_links = soup.find_all('a', href=True)
for link in all_links:
    href = link.get('href')
    if not self._is_google_news_url(href) and self._looks_like_news_url(href):
        original_links.append(href)

# Procura meta tags Open Graph
meta_tags = soup.find_all('meta')
for meta in meta_tags:
    if meta.get('property') == 'og:url':
        url = meta.get('content')
        if not self._is_google_news_url(url):
            original_links.append(url)
```

#### **Estrat√©gia 4: JavaScript e Data Attributes**
```python
# Procura URLs em scripts JavaScript
scripts = soup.find_all('script')
for script in scripts:
    url_patterns = [
        r'https?://[^\s"\'<>]+\.(com|br|org|net)/[^\s"\'<>]*',
        r'url["\']?\s*:\s*["\']([^"\']+)["\']'
    ]
    # Extrai URLs dos padr√µes encontrados

# Procura atributos data-*
elements_with_data = soup.find_all(attrs=lambda attrs: 
    any(k.startswith('data-') for k in attrs.keys()))
```

### 3. **Valida√ß√£o de URLs**

#### **Verifica√ß√µes de Validade**
```python
def _is_valid_news_url(self, url):
    # URLs muito curtos n√£o s√£o not√≠cias
    if len(url) < 20:
        return False
    
    # N√£o pode ser do Google News
    if self._is_google_news_url(url):
        return False
    
    # Deve parecer URL de not√≠cia
    return self._looks_like_news_url(url)

def _looks_like_news_url(self, url):
    news_patterns = [
        r'/\d{4}/\d{2}/\d{2}/',  # Data YYYY/MM/DD
        r'/noticia/',             # Cont√©m "noticia"
        r'/materia/',             # Cont√©m "materia"
        r'/artigo/',              # Cont√©m "artigo"
        r'\.html$',               # Termina com .html
        r'\.php$',                # Termina com .php
    ]
    # Verifica se URL corresponde a algum padr√£o
```

### 4. **Extra√ß√£o de Conte√∫do**

#### **Dados Extra√≠dos do Artigo Original**
```python
def _extract_content_from_url(self, url):
    extracted_data = {
        'url': url,
        'title': self._extract_title_from_soup(soup),
        'description': self._extract_description_from_soup(soup),
        'content': self._extract_content_from_soup(soup),
        'author': self._extract_author_from_soup(soup),
        'date': self._extract_date_from_soup(soup),
        'images': self._extract_images_from_soup(soup)
    }
    
    # Valida√ß√£o: conte√∫do deve ter pelo menos 200 caracteres
    if extracted_data['content'] and len(extracted_data['content']) > 200:
        return extracted_data
```

## Pontos Fortes do Sistema Atual

‚úÖ **M√∫ltiplas estrat√©gias** de extra√ß√£o de URL  
‚úÖ **Valida√ß√£o robusta** de URLs  
‚úÖ **Fallbacks inteligentes** quando falha  
‚úÖ **Headers realistas** para evitar bloqueios  
‚úÖ **Timeout adequado** para evitar travamentos  

## Poss√≠veis Melhorias Identificadas

üîç **Detec√ß√£o de relev√¢ncia** poderia ser mais sofisticada  
üîç **Cache de URLs** j√° extra√≠das para evitar reprocessamento  
üîç **Rate limiting** para evitar bloqueios por excesso de requests  
üîç **Retry logic** para URLs que falham temporariamente  
üîç **Logging mais detalhado** para debug  

## Pr√≥ximos Passos

1. **Revisar Passo 2**: Gera√ß√£o de T√≠tulo SEO
2. **Revisar Passo 3**: Cria√ß√£o de Conte√∫do com IA
3. **Revisar Passo 4**: Categoriza√ß√£o Inteligente
4. **Revisar Passo 5**: Recursos Multim√≠dia
5. **Revisar Passo 6**: Publica√ß√£o Final


